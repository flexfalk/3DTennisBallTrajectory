{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8100dc36",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:45.183601Z",
     "iopub.status.busy": "2024-05-27T09:55:45.183281Z",
     "iopub.status.idle": "2024-05-27T09:55:50.628767Z",
     "shell.execute_reply": "2024-05-27T09:55:50.627940Z"
    },
    "papermill": {
     "duration": 5.461352,
     "end_time": "2024-05-27T09:55:50.631165",
     "exception": false,
     "start_time": "2024-05-27T09:55:45.169813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import json\n",
    "from time import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9299fa34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:50.656484Z",
     "iopub.status.busy": "2024-05-27T09:55:50.656045Z",
     "iopub.status.idle": "2024-05-27T09:55:52.700989Z",
     "shell.execute_reply": "2024-05-27T09:55:52.699812Z"
    },
    "papermill": {
     "duration": 2.060076,
     "end_time": "2024-05-27T09:55:52.703443",
     "exception": false,
     "start_time": "2024-05-27T09:55:50.643367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TennisTrajectoryReconstruction'...\r\n",
      "remote: Enumerating objects: 441, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (162/162), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (102/102), done.\u001B[K\r\n",
      "remote: Total 441 (delta 60), reused 158 (delta 60), pack-reused 279\u001B[K\r\n",
      "Receiving objects: 100% (441/441), 5.40 MiB | 20.32 MiB/s, done.\r\n",
      "Resolving deltas: 100% (164/164), done.\r\n"
     ]
    }
   ],
   "source": [
    "username = \"\"\n",
    "password =  \"\"\n",
    "\n",
    "!git clone https://{username}:{password}@github.itu.dk/sosk/TennisTrajectoryReconstruction.git\n",
    "os.chdir('/kaggle/working/TennisTrajectoryReconstruction/')\n",
    "from TennisTrajectoryReconstruction.Utils.Reconstruction3D.Reconstruction3D_utils import create_3d_trajectory, create_3d_trajectory_with_spin, project_points_torch, get_court_dimension, average_distance, error_distance_landing, ball_hits_court, calculate_accuracy, calculate_f1_macro\n",
    "from TennisTrajectoryReconstruction.Utils.Visualisations.Visualisation_utils import plot_tennis_court\n",
    "from TennisTrajectoryReconstruction.Utils.CameraParams.CameraParams_utils import find_poles_and_corners_world\n",
    "os.chdir('/kaggle/working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8012d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.730811Z",
     "iopub.status.busy": "2024-05-27T09:55:52.730472Z",
     "iopub.status.idle": "2024-05-27T09:55:52.757782Z",
     "shell.execute_reply": "2024-05-27T09:55:52.756902Z"
    },
    "papermill": {
     "duration": 0.043526,
     "end_time": "2024-05-27T09:55:52.759810",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.716284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset_real(Dataset):\n",
    "    def __init__(self, input_data, camera_params, predicted):\n",
    "        self.input_data = input_data\n",
    "        self.camera_params = camera_params\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.homography_matrix = None\n",
    "        self.predicted = predicted\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        shot_id =  self.input_data.index[idx] - 1\n",
    "        \n",
    "        game_id = self.input_data.iloc[shot_id]['game']\n",
    "        clip_id = self.input_data.iloc[shot_id]['clip']\n",
    "        start = self.input_data.iloc[shot_id]['start']\n",
    "        end = self.input_data.iloc[shot_id]['end']\n",
    "        \n",
    "        \n",
    "        mtx, dist, rvecs, tvecs, rotation_matrix, homography = self.prepare_camera_parameters(self.camera_params, game_id, clip_id)\n",
    "\n",
    "        player1, player2 = self.find_players(game_id, clip_id, start, end)\n",
    "        #player1, player2, player1_homography, player2_homography = self.prepare_poses(game_id, clip_id, start, end)\n",
    "        homography_matrix = torch.tensor(self.homography_matrix, device=self.device)\n",
    "        \n",
    "        \n",
    "        if self.predicted:\n",
    "            x = eval(self.input_data.iloc[shot_id]['x_WASB'])\n",
    "            y = eval(self.input_data.iloc[shot_id]['y_WASB'])\n",
    "        else:\n",
    "            x = self.input_data.iloc[shot_id]['x_true']\n",
    "            y = self.input_data.iloc[shot_id]['y_true']\n",
    "            \n",
    "\n",
    "        \n",
    "        train = torch.tensor([[a/1280, b/720] for a, b in zip(x, y)], device=self.device)\n",
    "        \n",
    "        court_corners = self.get_corners(game_id, clip_id)\n",
    "        \n",
    "        if train.numel() == 0:\n",
    "            return None\n",
    "#         print(rotation_matrix)\n",
    "        # train.permute(1,0)\n",
    "        #return train, mtx, dist, rvecs, tvecs, rotation_matrix, player1, player2, player1_homography, player2_homography, court_corners, homography_matrix, (idx)\n",
    "        return train, mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), player1, player2, homography_matrix, game_id, clip_id\n",
    "    \n",
    "    def prepare_camera_parameters(self, camera_parameters, game_id, clip_id):\n",
    "        return_camera_params = self.camera_params[(self.camera_params['game'] == game_id) & (self.camera_params['clip'] == clip_id)].iloc[0]\n",
    "        \n",
    "        mtx = return_camera_params['camera_matrix']\n",
    "        dist = return_camera_params['dist']\n",
    "        rvecs = return_camera_params['rotation_vector']\n",
    "        tvecs = return_camera_params['translation_vector']\n",
    "        #print(type(eval(return_camera_params['homography_matrix'])))\n",
    "        homography_matrix = eval(return_camera_params['homography_matrix'])\n",
    "        \n",
    "        self.homography_matrix = np.array(homography_matrix)\n",
    "#         print(rvecs)\n",
    "        rotation_matrix, _ = cv2.Rodrigues(np.array(rvecs))\n",
    "\n",
    "        return (torch.tensor(mtx, requires_grad=True, device=self.device), torch.tensor(dist, requires_grad=True, device=self.device), torch.tensor(rvecs, requires_grad=True, device=self.device), \n",
    "                torch.tensor(tvecs, requires_grad=True, device=self.device), torch.tensor(rotation_matrix, requires_grad=True, device=self.device),\n",
    "                torch.tensor(homography_matrix, requires_grad=True, device=self.device))\n",
    "    \n",
    "        \n",
    "    def find_players(self, game_id, clip_id, start, end):\n",
    "        \n",
    "        \n",
    "        path = f\"/kaggle/input/filtered-poses/Dataset/{game_id}/{clip_id}/data.csv\"\n",
    "        pose_df = pd.read_csv(path).drop(columns=['Unnamed: 0', 'ball_x', 'ball_y', 'hits']).iloc[start:end].to_numpy()\n",
    "        \n",
    "        player1 = pose_df[:, 0:34]\n",
    "        player2 = pose_df[:, 34: ]\n",
    "        \n",
    "        \n",
    "        player1_left_foot = player1[:, 30:32]\n",
    "        player1_right_foot = player1[:, 32:34]\n",
    "        player1_mid = np.array(((player1_left_foot[:, 0] + player1_right_foot[:, 0]) / 2, (player1_left_foot[:, 1] + player1_right_foot[:, 1]) / 2)).T\n",
    "\n",
    "        player2_left_foot = player2[:, 30:32]\n",
    "        player2_right_foot = player2[:, 32:34]\n",
    "        player2_mid = np.array(((player2_left_foot[:, 0] + player2_right_foot[:, 0]) / 2, (player2_left_foot[:, 1] + player2_right_foot[:, 1]) / 2)).T\n",
    "        \n",
    "\n",
    "        \n",
    "        ground_plane_coords_player1 = cv2.perspectiveTransform(player1_mid[0].reshape(1, -1, 2), self.homography_matrix).squeeze()\n",
    "        ground_plane_coords_player2 = cv2.perspectiveTransform(player2_mid[0].reshape(1, -1, 2), self.homography_matrix).squeeze()\n",
    "\n",
    "        ground_plane_coords_player1 = np.append(ground_plane_coords_player1, 1.5)\n",
    "        ground_plane_coords_player2 = np.append(ground_plane_coords_player2, 1.5)\n",
    "        \n",
    "        court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "        \n",
    "\n",
    "\n",
    "        ground_plane_coords_player1[0] = ground_plane_coords_player1[0] / (half_court_width + 1)\n",
    "        ground_plane_coords_player1[1] = ground_plane_coords_player1[1] / (half_court_length + 1)\n",
    "        ground_plane_coords_player1[2] = ground_plane_coords_player1[2] / 3\n",
    "\n",
    "        ground_plane_coords_player2[0] = ground_plane_coords_player2[0] / (half_court_width + 1)\n",
    "        ground_plane_coords_player2[1] = ground_plane_coords_player2[1] / (half_court_length + 1)\n",
    "        ground_plane_coords_player2[2] = ground_plane_coords_player2[2] / 3\n",
    "        \n",
    "        return torch.tensor(ground_plane_coords_player1, requires_grad=True, device=self.device), torch.tensor(ground_plane_coords_player2, requires_grad=True, device=self.device)\n",
    "\n",
    "        \n",
    "    def get_corners(self, game_id, clip_id):\n",
    "        \n",
    "        path = f'/kaggle/input/court-detection-coordinates/Dataset/{game_id}/{clip_id}/court.csv'\n",
    "        \n",
    "        court_df = pd.read_csv(path)\n",
    "        court_coordinates = court_df[court_df['point'].isin([0, 1, 2, 3])][['x-coordinate', 'y-coordinate']].to_numpy().astype(np.float64)\n",
    "        \n",
    "        court_coordinates[:, 0] = court_coordinates[:, 0] / 1280\n",
    "        court_coordinates[:, 1] = court_coordinates[:, 1] / 720\n",
    "        \n",
    "        \n",
    "        return torch.tensor(court_coordinates, requires_grad=True, device=self.device,dtype=torch.float)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd345e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.786362Z",
     "iopub.status.busy": "2024-05-27T09:55:52.786084Z",
     "iopub.status.idle": "2024-05-27T09:55:52.806064Z",
     "shell.execute_reply": "2024-05-27T09:55:52.805309Z"
    },
    "papermill": {
     "duration": 0.03584,
     "end_time": "2024-05-27T09:55:52.807887",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.772047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset_Synthetic(Dataset):\n",
    "    def __init__(self, input_data, camera_params, sampled_cam_params = False):\n",
    "        self.input_data = input_data\n",
    "        self.camera_params = camera_params\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.homography_matrix = None\n",
    "        self.sampled_params = sampled_cam_params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        \n",
    "        shot_id =  self.input_data.index[idx] - 1\n",
    "        game_id = self.input_data.iloc[idx]['game']\n",
    "        clip_id = self.input_data.iloc[idx]['clip']\n",
    "        \n",
    "        starting_params = torch.tensor(eval(self.input_data.iloc[idx]['starting_params'])) \n",
    "        \n",
    "        \n",
    "        projection = torch.tensor(eval(self.input_data.iloc[idx]['projection']))\n",
    "        \n",
    "        projection[:, 0] = projection[:, 0] / 1280\n",
    "        projection[:, 1] = projection[:, 1] / 720\n",
    "        \n",
    "        start_position =  torch.tensor(eval(self.input_data.iloc[idx]['trajectory3D']))[0]\n",
    "        end_position =  torch.tensor(eval(self.input_data.iloc[idx]['trajectory3D']))[1]\n",
    "        \n",
    "        \n",
    "        court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "        \n",
    "        \n",
    "        \n",
    "        start_position[0] = start_position[0] / (half_court_width + 1)\n",
    "        start_position[1] = start_position[1] / (half_court_length + 1)\n",
    "        start_position[2] = start_position[2] / 3\n",
    "        \n",
    "        end_position[0] = end_position[0] / (half_court_width + 1)\n",
    "        end_position[1] = end_position[1] / (half_court_length + 1)\n",
    "        end_position[2] = end_position[2] / 3\n",
    "        \n",
    "        \n",
    "        mtx, dist, rvecs, tvecs, rotation_matrix, homography = self.prepare_camera_parameters(self.camera_params, game_id, clip_id)\n",
    "        if self.sampled_params:\n",
    "            court_coordinates = find_poles_and_corners_world()\n",
    "            corner = torch.tensor(court_coordinates[-4:])\n",
    "            court_corners = project_points_torch(corner, rotation_matrix, tvecs, mtx, dist)\n",
    "            \n",
    "        else:\n",
    "            court_corners = self.get_corners(game_id, clip_id)\n",
    "        \n",
    "        \n",
    "        #if train.numel() == 0:\n",
    "        #    return None\n",
    "#         print(rotation_matrix)\n",
    "        # train.permute(1,0)\n",
    "\n",
    "        return starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), start_position, end_position, homography, game_id, clip_id\n",
    "    \n",
    "    def prepare_camera_parameters(self, camera_parameters, game_id, clip_id):\n",
    "        return_camera_params = self.camera_params[(self.camera_params['game'] == game_id) & (self.camera_params['clip'] == clip_id)].iloc[0]\n",
    "        \n",
    "        mtx = return_camera_params['camera_matrix']\n",
    "        dist = return_camera_params['dist']\n",
    "        rvecs = return_camera_params['rotation_vector']\n",
    "        tvecs = return_camera_params['translation_vector']\n",
    "        #print(type(eval(return_camera_params['homography_matrix'])))\n",
    "        homography_matrix = eval(return_camera_params['homography_matrix'])\n",
    "        \n",
    "        self.homography_matrix = np.array(homography_matrix)\n",
    "#         print(rvecs)\n",
    "        rotation_matrix, _ = cv2.Rodrigues(np.array(rvecs))\n",
    "\n",
    "        return (torch.tensor(mtx, requires_grad=True, device=self.device), torch.tensor(dist, requires_grad=True, device=self.device), torch.tensor(rvecs, requires_grad=True, device=self.device), \n",
    "                torch.tensor(tvecs, requires_grad=True, device=self.device), torch.tensor(rotation_matrix, requires_grad=True, device=self.device),\n",
    "                torch.tensor(homography_matrix, requires_grad=True, device=self.device))\n",
    "    \n",
    "    def get_corners(self, game_id, clip_id):\n",
    "        \n",
    "        path = f'/kaggle/input/court-detection-coordinates/Dataset/{game_id}/{clip_id}/court.csv'\n",
    "        \n",
    "        court_df = pd.read_csv(path)\n",
    "        court_coordinates = court_df[court_df['point'].isin([0, 1, 2, 3])][['x-coordinate', 'y-coordinate']].to_numpy().astype(np.float64)\n",
    "        \n",
    "        court_coordinates[:, 0] = court_coordinates[:, 0] / 1280\n",
    "        court_coordinates[:, 1] = court_coordinates[:, 1] / 720\n",
    "        \n",
    "        \n",
    "        return torch.tensor(court_coordinates, requires_grad=True, device=self.device,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4ba925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.834223Z",
     "iopub.status.busy": "2024-05-27T09:55:52.833936Z",
     "iopub.status.idle": "2024-05-27T09:55:52.850604Z",
     "shell.execute_reply": "2024-05-27T09:55:52.849801Z"
    },
    "papermill": {
     "duration": 0.03195,
     "end_time": "2024-05-27T09:55:52.852574",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.820624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_tensor(tensor, desired_rows):\n",
    "    current_rows = tensor.shape[0]\n",
    "    rows_to_pad = desired_rows - current_rows\n",
    "    return F.pad(tensor, (0, 0, 0, rows_to_pad), value=-1, mode='constant')\n",
    "\n",
    "\n",
    "# Define a custom converter function\n",
    "def parse_list(string):\n",
    "    try:\n",
    "        # Use ast.literal_eval to safely parse the string representation of a list\n",
    "        return ast.literal_eval(string)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # If parsing fails, return None or any other default value as needed\n",
    "        return None\n",
    "    \n",
    "def custom_collate_synthetic(batch, GRU):\n",
    "    # Extract data from the batch\n",
    "    starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), start_position, end_position, homography, game_id, clip_id = zip(*batch)\n",
    "    \n",
    "    # Pad inputs differently for each batch\n",
    "    #max_input_length = max(len(seq[:, 1]) for seq in inputs)\n",
    "    #padded_projection = [torch.nn.functional.pad(seq, (0, 0, 0, 50 - len(seq[:, 1])), value=-1, mode='constant') for seq in projection]\n",
    "    #padded_projection = torch.stack(padded_projection)\n",
    "    if GRU:\n",
    "        max_input_length = max(len(seq[:, 1]) for seq in inputs)\n",
    "        padded_projection = [torch.nn.functional.pad(seq, (0, 0, 0, max_input_length - len(seq[:, 1])), value=-1, mode='constant') for seq in projection]\n",
    "        padded_projection = torch.stack(padded_projection)\n",
    "    else:\n",
    "        padded_inputs = [pad_tensor(seq, 50) for seq in projection]\n",
    "        #padded_projection = [torch.nn.functional.pad(seq, (0, 0, 0, 50 - len(seq[:, 1])), value=-1, mode='constant') for seq in projection]\n",
    "        padded_projection = torch.stack(padded_inputs)\n",
    "\n",
    "    \n",
    "    # Stack other data\n",
    "    stacked_mtx = torch.stack(mtx).to(device)\n",
    "    stacked_dist = torch.stack(dist).to(device)\n",
    "    stacked_rvecs = torch.stack(rvecs).to(device)\n",
    "    stacked_tvecs = torch.stack(tvecs).to(device)\n",
    "    stacked_rotation_matrix = torch.stack(rotation_matrix).to(device)\n",
    "    stacked_starting_params = torch.stack(starting_params).to(device)\n",
    "    stacked_start_position = torch.stack(start_position).to(device)\n",
    "    stacked_end_position = torch.stack(end_position).to(device)\n",
    "    stacked_homography = torch.stack(homography).to(device)\n",
    "    #game_id = torch.stack(game_id)\n",
    "    #clip_id = torch.stack(clip_id)\n",
    "    \n",
    "    #stacked_player1 = torch.stack(player1)\n",
    "    #stacked_player2 = torch.stack(player2)\n",
    "    stacked_court_corners = torch.stack(court_corners).to(device)\n",
    "    \n",
    "    # Return padded inputs and other data\n",
    "    return (stacked_starting_params, padded_projection, stacked_mtx, stacked_dist, stacked_rvecs, stacked_tvecs, stacked_rotation_matrix, \n",
    "            stacked_court_corners, shot_id, stacked_start_position, stacked_end_position, stacked_homography, game_id, clip_id)\n",
    "\n",
    "# Custom collate function to handle sequences of varying lengths\n",
    "def custom_collate_real(batch, GRU):\n",
    "    for i in batch:\n",
    "        if i is None:\n",
    "            return None\n",
    "    inputs, mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), player1, player2, homography_matrix, game_id, clip_id = zip(*batch)\n",
    "    if GRU:\n",
    "        max_input_length = max(len(seq[:, 1]) for seq in inputs)\n",
    "        padded_inputs = [torch.nn.functional.pad(seq, (0, 0, 0, max_input_length - len(seq[:, 1])), value=-1, mode='constant') for seq in inputs]\n",
    "        padded_inputs = torch.stack(padded_inputs)\n",
    "    else:\n",
    "        padded_inputs = [pad_tensor(seq, 50) for seq in inputs]\n",
    "        padded_inputs = torch.stack(padded_inputs)\n",
    "\n",
    "    return (padded_inputs, torch.stack(mtx), torch.stack(dist), torch.stack(rvecs), torch.stack(tvecs), \n",
    "            torch.stack(rotation_matrix), torch.stack(court_corners), shot_id, torch.stack(player1), torch.stack(player2), torch.stack(homography_matrix), game_id, clip_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1da12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.878339Z",
     "iopub.status.busy": "2024-05-27T09:55:52.878079Z",
     "iopub.status.idle": "2024-05-27T09:55:52.892278Z",
     "shell.execute_reply": "2024-05-27T09:55:52.891462Z"
    },
    "papermill": {
     "duration": 0.029332,
     "end_time": "2024-05-27T09:55:52.894050",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.864718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise_starting_params(starting_params : str):\n",
    "    \n",
    "    court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "    starting_params = np.array(eval(starting_params))\n",
    "    \n",
    "    starting_params[0] = starting_params[0] / (half_court_width + 1)\n",
    "    starting_params[1] = starting_params[1] / (half_court_length + 1)\n",
    "    starting_params[2] = starting_params[2] / 3\n",
    "    starting_params[3] = starting_params[3] / 4\n",
    "    starting_params[4] = starting_params[4] / 30\n",
    "    starting_params[5] = starting_params[5] / 5\n",
    "    \n",
    "    if len(starting_params) > 6:\n",
    "        starting_params[6] = starting_params[6] / (10*2*math.pi)\n",
    "        starting_params[7] = starting_params[7] / (1*2*math.pi)\n",
    "        starting_params[8] = starting_params[8] / (5*2*math.pi)\n",
    "    \n",
    "    return str(list(starting_params))\n",
    "\n",
    "def upscale_starting_params(starting_params : torch.tensor):\n",
    "    court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "    \n",
    "    starting_params[0] = starting_params[0] * (half_court_width + 1)\n",
    "    starting_params[1] = starting_params[1] * (half_court_length + 1)\n",
    "    starting_params[2] = starting_params[2] * 3\n",
    "    starting_params[3] = starting_params[3] * 4\n",
    "    starting_params[4] = starting_params[4] * 30\n",
    "    starting_params[5] = starting_params[5] * 5\n",
    "    \n",
    "    \n",
    "    if len(starting_params) > 6:\n",
    "        starting_params[6] = starting_params[6] * (10*2*math.pi)\n",
    "        starting_params[7] = starting_params[7] * (1*2*math.pi)\n",
    "        starting_params[8] = starting_params[8] * (5*2*math.pi)\n",
    "    \n",
    "    return starting_params\n",
    "\n",
    "def upscale_starting_params_batch(starting_params : torch.tensor):\n",
    "    court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "    \n",
    "    starting_params[:,0] = starting_params[:,0] * (half_court_width + 1)\n",
    "    starting_params[:,1] = starting_params[:,1] * (half_court_length + 1)\n",
    "    starting_params[:,2] = starting_params[:,2] * 3\n",
    "    starting_params[:,3] = starting_params[:,3] * 4\n",
    "    starting_params[:,4] = starting_params[:,4] * 30\n",
    "    starting_params[:,5] = starting_params[:,5] * 5\n",
    "    \n",
    "    if starting_params.shape[1] > 6:\n",
    "        starting_params[:,6] = starting_params[:,6] * (10*2*math.pi)\n",
    "        starting_params[:,7] = starting_params[:,7] * (1*2*math.pi)\n",
    "        starting_params[:,8] = starting_params[:,8] * (5*2*math.pi)\n",
    "    \n",
    "    return starting_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f39ee7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.919777Z",
     "iopub.status.busy": "2024-05-27T09:55:52.919483Z",
     "iopub.status.idle": "2024-05-27T09:55:52.932974Z",
     "shell.execute_reply": "2024-05-27T09:55:52.932262Z"
    },
    "papermill": {
     "duration": 0.028435,
     "end_time": "2024-05-27T09:55:52.934721",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.906286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_trajectory(traj):\n",
    "    \n",
    "    \n",
    "    if traj[0] <= 0:\n",
    "        vals_to_change = []\n",
    "        i = 0\n",
    "        \n",
    "        while traj[i] <= 0:\n",
    "            vals_to_change.append(i)\n",
    "            i += 1\n",
    "        \n",
    "        for j in vals_to_change:\n",
    "            traj[j] = traj[i]\n",
    "        \n",
    "            \n",
    "    \n",
    "    for i in range(len(traj)):\n",
    "        if traj[i] <= 0:\n",
    "            traj[i] = traj[i-1]\n",
    "        \n",
    "    return traj\n",
    "\n",
    "\n",
    "def clean_true(df):\n",
    "    \n",
    "    v_to_remove = [\"nan\", \" \", \" nan\"]\n",
    "    df[\"x_true\"] = df.apply(lambda row: [float(v) if v not in v_to_remove else 0 for v in row[\"x_true\"].strip(\"[]\").split(\", \")], axis=1)\n",
    "    df[\"y_true\"] = df.apply(lambda row: [float(v) if v not in v_to_remove else 0 for v in row[\"y_true\"].strip(\"[]\").split(\", \")], axis=1)\n",
    "    \n",
    "    df['x_true'] = df['x_true'].apply(lambda row : fix_trajectory(row))\n",
    "    df['y_true'] = df['y_true'].apply(lambda row : fix_trajectory(row))\n",
    "    df = df.reset_index()\n",
    "    df = df.drop([432, 131, 483, 233, 440, 6, 420, 34, 317, 451, 272, 438, 342, 430])\n",
    "    print(len(df))\n",
    "    df = df[df['x_true'].apply(lambda row : len(row) > 5)]\n",
    "    print(len(df))\n",
    "    return df.reset_index()\n",
    "\n",
    "def clean_predicted(ball_df):\n",
    "    ball_df = ball_df[ball_df[\"x_WASB\"].apply(lambda x: len(x)  > 3)].reset_index()\n",
    "    ball_df = ball_df.drop([293, 141 , 462, 460, 443, 471, 35, 364, 6])\n",
    "    ball_df['x_WASB'] = ball_df['x_WASB'].apply(lambda row : str(fix_trajectory(eval(row))))\n",
    "    ball_df['y_WASB'] = ball_df['y_WASB'].apply(lambda row : str(fix_trajectory(eval(row))))\n",
    "    print(len(ball_df))\n",
    "    ball_df = ball_df[ball_df['x_WASB'].apply(lambda row : len(eval(row)) > 5)]\n",
    "    print(len(ball_df))\n",
    "    return ball_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711459d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:52.960175Z",
     "iopub.status.busy": "2024-05-27T09:55:52.959882Z",
     "iopub.status.idle": "2024-05-27T09:55:55.102410Z",
     "shell.execute_reply": "2024-05-27T09:55:55.101544Z"
    },
    "papermill": {
     "duration": 2.15844,
     "end_time": "2024-05-27T09:55:55.105113",
     "exception": false,
     "start_time": "2024-05-27T09:55:52.946673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n",
      "487\n",
      "508\n",
      "479\n"
     ]
    }
   ],
   "source": [
    "convert_dict = {'camera_matrix': parse_list,\n",
    "               'dist': parse_list,\n",
    "               'rotation_vector': parse_list,\n",
    "               'translation_vector':parse_list}\n",
    "\n",
    "camera_params_train  = pd.read_csv('/kaggle/input/camera-parameters/camera_paramters.csv', \n",
    "                      converters=convert_dict).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "ball_df_true = clean_true(pd.read_csv('/kaggle/input/segmented-shots-predictions/true_segmented_shots.csv', \n",
    "                      converters=convert_dict).drop(columns=['Unnamed: 0']))\n",
    "\n",
    "ball_df_pred = clean_predicted(pd.read_csv('/kaggle/input/segmented-shots-predictions/predicted_segmented_shots.csv', \n",
    "                      converters=convert_dict).drop(columns=['Unnamed: 0']))\n",
    "\n",
    "synthetic_df = pd.read_csv('/kaggle/input/synthetic-tennis-trajectories/10000_synthetic_shots.csv', \n",
    "                      converters=convert_dict).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "synthetic_df_spin = pd.read_csv('/kaggle/input/synthetic-tennis-trajectories/10000_synthetic_shots_spin.csv', \n",
    "                      converters=convert_dict).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "synthetic_df_spin.loc[:, 'starting_params'] = synthetic_df_spin['starting_params'].apply(lambda row: normalise_starting_params(row))\n",
    "\n",
    "synthetic_df.loc[:, 'starting_params'] = synthetic_df['starting_params'].apply(lambda row: normalise_starting_params(row))\n",
    "\n",
    "# Synthetic without spin\n",
    "batch_size = 64\n",
    "synthetic_train = synthetic_df[~synthetic_df['game'].isin(['game1', 'game8'])]\n",
    "syn_dataset_train = CustomDataset_Synthetic(synthetic_train, camera_params_train)\n",
    "syn_loader_train_NN = DataLoader(syn_dataset_train, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: custom_collate_synthetic(batch, GRU=False))\n",
    "\n",
    "synthetic_test = synthetic_df[synthetic_df['game'].isin(['game1', 'game8'])]\n",
    "syn_dataset_val = CustomDataset_Synthetic(synthetic_test, camera_params_train)\n",
    "syn_loader_val_NN = DataLoader(syn_dataset_val, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: custom_collate_synthetic(batch, GRU=False))\n",
    "\n",
    "# Synthetic without spin\n",
    "batch_size = 64\n",
    "synthetic_train_spin = synthetic_df_spin[~synthetic_df_spin['game'].isin(['game1', 'game8'])]\n",
    "syn_dataset_train_spin = CustomDataset_Synthetic(synthetic_train_spin, camera_params_train)\n",
    "syn_loader_train_NN_spin = DataLoader(syn_dataset_train_spin, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: custom_collate_synthetic(batch, GRU=False))\n",
    "\n",
    "synthetic_test_spin = synthetic_df_spin[synthetic_df_spin['game'].isin(['game1', 'game8'])]\n",
    "syn_dataset_val_spin = CustomDataset_Synthetic(synthetic_test_spin, camera_params_train)\n",
    "syn_loader_val_NN_spin = DataLoader(syn_dataset_val_spin, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: custom_collate_synthetic(batch, GRU=False))\n",
    "\n",
    "\n",
    "# Real data\n",
    "train_df_true = ball_df_true[~ball_df_true['game'].isin(['game1', 'game8'])]\n",
    "real_train_dataset_true = CustomDataset_real(train_df_true, camera_params_train, predicted = False)\n",
    "real_train_loader_NN_true = DataLoader(real_train_dataset_true, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n",
    "\n",
    "train_df_pred = ball_df_pred[~ball_df_pred['game'].isin(['game1', 'game8'])]\n",
    "real_train_dataset_pred = CustomDataset_real(train_df_pred, camera_params_train, predicted = True)\n",
    "real_train_loader_NN_pred = DataLoader(real_train_dataset_pred, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n",
    "\n",
    "# real test\n",
    "test_df_true = ball_df_true[ball_df_true['game'].isin(['game1', 'game8'])]\n",
    "real_test_dataset_true = CustomDataset_real(test_df_true, camera_params_train, predicted = False)\n",
    "real_test_loader_NN_true = DataLoader(real_test_dataset_true, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n",
    "\n",
    "test_df_pred = ball_df_pred[ball_df_pred['game'].isin(['game1', 'game8'])]\n",
    "real_test_dataset_pred = CustomDataset_real(test_df_pred, camera_params_train,predicted = True)\n",
    "real_test_loader_NN_pred = DataLoader(real_test_dataset_pred, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n",
    "\n",
    "# real all\n",
    "real_all_dataset_true = CustomDataset_real(ball_df_true, camera_params_train, predicted = False)\n",
    "real_all_loader_NN_true = DataLoader(real_all_dataset_true, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n",
    "\n",
    "real_all_dataset_pred = CustomDataset_real(ball_df_pred, camera_params_train,predicted = True)\n",
    "real_all_loader_NN_pred = DataLoader(real_all_dataset_pred, batch_size=1, shuffle=True, collate_fn=lambda batch : custom_collate_real(batch, GRU=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee32d8f",
   "metadata": {
    "papermill": {
     "duration": 0.012627,
     "end_time": "2024-05-27T09:55:55.157680",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.145053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca31bb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:55.184914Z",
     "iopub.status.busy": "2024-05-27T09:55:55.184297Z",
     "iopub.status.idle": "2024-05-27T09:55:55.198577Z",
     "shell.execute_reply": "2024-05-27T09:55:55.197659Z"
    },
    "papermill": {
     "duration": 0.030382,
     "end_time": "2024-05-27T09:55:55.200544",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.170162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob=0.2):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc21 = nn.Linear(512, 1028)\n",
    "        self.fc22 = nn.Linear(1028, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "        self.final = nn.Linear(128, output_size)\n",
    "        \n",
    "        # Define batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn21 =  nn.BatchNorm1d(1028)\n",
    "        self.bn22 =  nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.dropout21 = nn.Dropout(dropout_prob)\n",
    "        self.dropout22 = nn.Dropout(dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Weight initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # without normalization\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn21(self.fc21(x)))\n",
    "        x = self.dropout21(x)\n",
    "        x = torch.relu(self.bn22(self.fc22(x)))\n",
    "        x = self.dropout22(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f437051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:55.227007Z",
     "iopub.status.busy": "2024-05-27T09:55:55.226705Z",
     "iopub.status.idle": "2024-05-27T09:55:55.252781Z",
     "shell.execute_reply": "2024-05-27T09:55:55.252048Z"
    },
    "papermill": {
     "duration": 0.042021,
     "end_time": "2024-05-27T09:55:55.254923",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.212902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_syn(num_epochs : int, lr : float, use_court : bool, use_cam_params : bool, use_positions : bool, spin : bool, model_):\n",
    "\n",
    "    input_size = 100\n",
    "    output_size = 6\n",
    "    \n",
    "    data_loader = syn_loader_train_NN\n",
    "    val_loader = syn_loader_val_NN\n",
    "    \n",
    "    if spin:\n",
    "        output_size += 3\n",
    "        data_loader = syn_loader_train_NN_spin\n",
    "        val_loader = syn_loader_val_NN_spin\n",
    "    if use_court:\n",
    "        input_size += 4*2\n",
    "    if use_cam_params:\n",
    "        input_size += 3*3 + 3 + 3*3 + 5\n",
    "    if use_positions:\n",
    "        input_size += 3*2\n",
    "    \n",
    "    losses = []\n",
    "    val_loss = []\n",
    "    mse_pr_items = []\n",
    "\n",
    "\n",
    "    model = model_(input_size, output_size).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        predicted_tiles = []\n",
    "        true_tiles = []\n",
    "        \n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "        # Iterate over batches in the train_loader\n",
    "        for batch in tqdm(data_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "\n",
    "            starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), start_position, end_position, homography, game_id, clip_id = batch\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            concatted_input = projection.view(-1, 2*50).to(device)\n",
    "            \n",
    "            if use_court:\n",
    "                concatted_court = court_corners.view(-1, 4*2).to(starting_params.device)\n",
    "                concatted_input = torch.cat((concatted_court, concatted_input), dim=1)\n",
    "            if use_cam_params:\n",
    "                camera_matrix = mtx.view(-1, 3*3).to(starting_params.device)\n",
    "                translation_matrix = tvecs.view(-1, 3).to(starting_params.device)\n",
    "                rotation = rotation_matrix.view(-1, 3*3).to(starting_params.device)\n",
    "                distortion = dist.view(-1, 5).to(starting_params.device)\n",
    "                concatted_params = torch.cat((camera_matrix, translation_matrix, rotation, distortion), dim=1)\n",
    "                concatted_input = torch.cat((concatted_params, concatted_input), dim=1).to(starting_params.device)\n",
    "            if use_positions:\n",
    "                concatted_positions = torch.cat((start_pos, end_pos)).view(-1, 3*2)\n",
    "                concatted_input = torch.cat((concatted_positions, concatted_input), dim=1).to(starting_params.device)\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            # Forward pass\n",
    "\n",
    "            outputs = model(concatted_input.float())\n",
    "            \n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, starting_params)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "                starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), start_position, end_position, homography, game_id, clip_id = batch\n",
    "            \n",
    "            \n",
    "                concatted_input = projection.view(-1, 2*50).to(device)\n",
    "\n",
    "                if use_court:\n",
    "                    concatted_court = court_corners.view(-1, 4*2).to(starting_params.device)\n",
    "                    concatted_input = torch.cat((concatted_court, concatted_input), dim=1)\n",
    "                if use_cam_params:\n",
    "                    camera_matrix = mtx.view(-1, 3*3).to(starting_params.device)\n",
    "                    translation_matrix = tvecs.view(-1, 3).to(starting_params.device)\n",
    "                    rotation = rotation_matrix.view(-1, 3*3).to(starting_params.device)\n",
    "                    distortion = dist.view(-1, 5).to(starting_params.device)\n",
    "                    concatted_params = torch.cat((camera_matrix, translation_matrix, rotation, distortion), dim=1)\n",
    "                    concatted_input = torch.cat((concatted_params, concatted_input), dim=1).to(starting_params.device)\n",
    "                if use_positions:\n",
    "                    concatted_positions = torch.cat((start_pos, end_pos)).view(-1, 3*2)\n",
    "                    concatted_input = torch.cat((concatted_positions, concatted_input), dim=1).to(starting_params.device)\n",
    "                    \n",
    "                outputs = model(concatted_input.float())\n",
    "                #outputs = upscale_starting_params_batch(outputs)\n",
    "                \n",
    "                #labels = upscale_starting_params_batch(starting_params)\n",
    "                #labels = \n",
    "                loss = torch.sqrt(criterion(outputs, starting_params))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate MSE per item in each batch\n",
    "                mse_per_item = torch.sqrt(torch.mean((outputs - starting_params) ** 2, dim=0))\n",
    "                mse_pr_items.append(mse_per_item)\n",
    "                \n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / len(syn_loader_val_NN)\n",
    "        loss_pr_item = torch.mean(torch.stack(mse_pr_items),dim=0)# / len(syn_loader_val_NN)\n",
    "        print(loss_pr_item)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(syn_loader_train_NN.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}')\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bbaaaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:55.281885Z",
     "iopub.status.busy": "2024-05-27T09:55:55.281587Z",
     "iopub.status.idle": "2024-05-27T09:55:55.321335Z",
     "shell.execute_reply": "2024-05-27T09:55:55.320578Z"
    },
    "papermill": {
     "duration": 0.055616,
     "end_time": "2024-05-27T09:55:55.323284",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.267668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_NN(data_loader : DataLoader, use_court : bool, use_cam_params : bool, use_positions : bool, synthetic_data : bool, spin : bool, model):\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    game_n = []\n",
    "    clip_n = []\n",
    "    shot_n = []\n",
    "    mse_n = []\n",
    "    rmse_n = []\n",
    "    pred_tiles = [] \n",
    "    true_tiles = []\n",
    "    landing_error = []\n",
    "    predicted_3D_traj = []\n",
    "    start_errors = []\n",
    "    \n",
    "    trajectory_distance = []\n",
    "\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            \n",
    "            if not synthetic_data:\n",
    "                projection, mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, shot_id, player1, player2, homography, game_id, clip_id = batch\n",
    "            else:\n",
    "                starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, (shot_id), start_position, end_position, homography, game_id, clip_id = batch\n",
    "                \n",
    "            \n",
    "            concatted_input = projection.view(-1, 2*50).to(device)\n",
    "            \n",
    "            if use_court:\n",
    "                concatted_court = court_corners.view(-1, 4*2)#.to(projection.device)\n",
    "                \n",
    "                concatted_input = torch.cat((concatted_court, concatted_input), dim=1)\n",
    "                \n",
    "            if use_cam_params:\n",
    "                camera_matrix = mtx.view(-1, 3*3)\n",
    "                translation_matrix = tvecs.view(-1, 3)\n",
    "                rotation = rotation_matrix.view(-1, 3*3)\n",
    "                distortion = dist.view(-1, 5)\n",
    "                concatted_params = torch.cat((camera_matrix, translation_matrix, rotation, distortion), dim=1)\n",
    "                concatted_input = torch.cat((concatted_params, concatted_input), dim=1)\n",
    "            if use_positions:\n",
    "                concatted_players = torch.cat((player1, player2)).view(-1, 6)\n",
    "                concatted_input = torch.cat((concatted_players, concatted_input), dim=1)\n",
    "                \n",
    "            #print(concatted_input.float().device)\n",
    "\n",
    "            preds = model(concatted_input.float())\n",
    "\n",
    "\n",
    "            for i in range(len(batch[0])):\n",
    "                \n",
    "                game_n.append(game_id[i])\n",
    "                clip_n.append(clip_id[i])\n",
    "                shot_n.append(shot_id[i])\n",
    "                \n",
    "                #camera_matrices.append(mtx[i])\n",
    "                #distortion_coefficients.append(dist[i])\n",
    "                #translation_vectors.append(tvecs[i])\n",
    "                #rotational_matrices.append(rotation_matrix[i])\n",
    "                \n",
    "                if synthetic_data:\n",
    "                    \n",
    "                    \n",
    "                    curr_input = projection[i]\n",
    "\n",
    "                    mask = curr_input != -1\n",
    "                    labels = curr_input[mask].view(1,-1,2).to(device)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    labels[:,:, 0] *= 1280\n",
    "                    labels[:,:,1] *= 720\n",
    "                    #true_trajectories.append(labels)\n",
    "                    \n",
    "                    \n",
    "                    N = labels.shape[1]\n",
    "                    \n",
    "                    prediction = upscale_starting_params(preds[i])\n",
    "                    true_start = upscale_starting_params(starting_params[i])\n",
    "                    \n",
    "                    squarred_error = (preds[i]-starting_params[i])**2\n",
    "                    start_errors.append([i.item() for i in squarred_error])\n",
    "                    \n",
    "                    \n",
    "                    if spin:\n",
    "                        pred_traj = create_3d_trajectory_with_spin(prediction.view(1, 9), N)\n",
    "                        true_3D_traj = create_3d_trajectory_with_spin(true_start.view(1, 9), N)\n",
    "                    else:\n",
    "                        pred_traj = create_3d_trajectory(prediction.view(1, 6), N)\n",
    "                        true_3D_traj = create_3d_trajectory(true_start.view(1, 6), N)\n",
    "                            \n",
    "                    predicted_3D_traj.append(pred_traj.cpu().tolist())\n",
    "                    pred_projected_path = project_points_torch(pred_traj.to(device), rotation_matrix[i].to(device), tvecs[i].to(device), mtx[i].to(device), dist[i].to(device))\n",
    "                    \n",
    "\n",
    "                    pred_tile, true_tile = ball_hits_court(pred_traj, labels, homography[i].to(device))\n",
    "                    pred_tiles.append(pred_tile)\n",
    "                    true_tiles.append(true_tile)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    lan_error = error_distance_landing(pred_traj, labels, homography[i].to(device))\n",
    "                    landing_error.append(lan_error)\n",
    "                    \n",
    "                    mse = mse_loss(pred_projected_path, labels[0])\n",
    "                    mse_n.append(mse.item())\n",
    "                    rmse = torch.sqrt(mse)\n",
    "                    rmse_n.append(rmse.item())\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    avg_dist = average_distance(pred_traj.cpu(), true_3D_traj.cpu())\n",
    "                    trajectory_distance.append(avg_dist)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    curr_input = projection[i]\n",
    "\n",
    "\n",
    "                    mask = curr_input != -1\n",
    "                    non_padded_input = curr_input[mask].view(1,-1,2)\n",
    "\n",
    "\n",
    "                    N = non_padded_input.shape[1]\n",
    "\n",
    "\n",
    "                    labels = non_padded_input#.detach().cpu().numpy()\n",
    "\n",
    "                    #true_trajectories.append(labels)\n",
    "\n",
    "                    labels[:,:, 0] *= 1280\n",
    "                    labels[:,:,1] *= 720\n",
    "\n",
    "\n",
    "                    prediction = upscale_starting_params(preds[i])\n",
    "                    #outputs.append(prediction)\n",
    "                    \n",
    "                    if spin:\n",
    "                        new_traj = create_3d_trajectory_with_spin(prediction.view(1, 9), N)\n",
    "                    else:\n",
    "                        new_traj = create_3d_trajectory(prediction.view(1, 6), N)\n",
    "                \n",
    "                    predicted_3D_traj.append(new_traj.cpu().tolist())\n",
    "                    projected_path = project_points_torch(new_traj, rotation_matrix[i], tvecs[i], mtx[i], dist[i])\n",
    "                    \n",
    "                    \n",
    "                    pred_tile, true_tile = ball_hits_court(new_traj, labels, homography[i].to(device))\n",
    "                    pred_tiles.append(pred_tile)\n",
    "                    true_tiles.append(true_tile)\n",
    "\n",
    "                    lan_error = error_distance_landing(new_traj, labels, homography[i].to(device))\n",
    "                    landing_error.append(lan_error)\n",
    "                    \n",
    "                    mse = mse_loss(projected_path, labels[0])\n",
    "                    mse_n.append(mse.item())\n",
    "                    rmse = torch.sqrt(mse)\n",
    "                    rmse_n.append(rmse.item())\n",
    "\n",
    "                    \n",
    "    if not synthetic_data:\n",
    "        result_df = pd.DataFrame(list(zip(game_n, clip_n, shot_n, mse_n, rmse_n, pred_tiles, true_tiles, landing_error, predicted_3D_traj)),\n",
    "                   columns =['Game', 'Clip', \"Shot Number\", \"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error', 'predicted_trajectory'])\n",
    "\n",
    "\n",
    "        grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n",
    "        \"acc\" : calculate_accuracy(group), \n",
    "        \"f1\" : calculate_f1_macro(group), \n",
    "        \"MSE\" : group[\"MSE\"].mean(),\n",
    "        \"RMSE\" : group[\"RMSE\"].mean(),\n",
    "        'Landing Error' : group['landing_error'].mean()}))\n",
    "        #print(result_df['MSE'])\n",
    "\n",
    "        mean_results = { \"mean mse\": np.mean(result_df[\"MSE\"].tolist()), \n",
    "                        \"mean rmse\" : np.mean(result_df[\"RMSE\"].tolist()),\n",
    "                        \"acc\" : accuracy_score(result_df[\"predicted_tiles\"], result_df[\"true_tiles\"]), \n",
    "                        \"f1\" : f1_score(result_df[\"predicted_tiles\"], result_df[\"true_tiles\"], average='macro'),\n",
    "                       'mean landing_error' : np.mean(result_df['landing_error'].tolist())}\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        result_df = pd.DataFrame(list(zip(game_n, clip_n, shot_n, mse_n, rmse_n, pred_tiles, true_tiles, landing_error, trajectory_distance, start_errors, predicted_3D_traj)),\n",
    "                   columns =['Game', 'Clip', \"Shot Number\", \"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error', 'trajectory_distance', 'start_error','predicted_trajectory'])\n",
    "        \n",
    "        mean_start_errors = np.mean(np.array(start_errors), axis=0).tolist()\n",
    "        \n",
    "        grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n",
    "            'acc': calculate_accuracy(group),\n",
    "            'f1': calculate_f1_macro(group),\n",
    "            'MSE': group['MSE'].mean(),\n",
    "            'RMSE': group['RMSE'].mean(),\n",
    "            'Landing Error': group['landing_error'].mean(),\n",
    "            'Trajectory Distance': group['trajectory_distance'].mean(),\n",
    "            'Start Error': np.sqrt(np.mean(np.array(group['start_error'].tolist()), axis=0)).tolist()\n",
    "        }))\n",
    "        \n",
    "        print(grouped['Start Error'])\n",
    "        \n",
    "         # Grouped metrics calculation\n",
    "        #grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n",
    "        #    'acc': calculate_accuracy(group),\n",
    "        #    'f1': calculate_f1_macro(group),\n",
    "        #    'MSE': group['MSE'].mean(),\n",
    "        #    'RMSE': group['RMSE'].mean(),\n",
    "        #    'Landing Error': group['landing_error'].mean(),\n",
    "        #    'Trajectory Distance': group['trajectory_distance'].mean(),\n",
    "        #    'Start Error': group['start_error'].apply(lambda x: np.mean(x))\n",
    "        #}))\n",
    "        \n",
    "\n",
    "        mean_results = {\n",
    "            'mean mse': np.mean(result_df['MSE'].tolist()),\n",
    "            'mean rmse': np.mean(result_df['RMSE'].tolist()),\n",
    "            'acc': accuracy_score(result_df['predicted_tiles'], result_df['true_tiles']),\n",
    "            'f1': f1_score(result_df['predicted_tiles'], result_df['true_tiles'], average='macro'),\n",
    "            'mean landing error': np.mean(result_df['landing_error'].tolist()),\n",
    "            'mean trajectory distance': np.mean(result_df['trajectory_distance'].tolist()),\n",
    "            'mean start error': mean_start_errors\n",
    "        }\n",
    "        print('could find mean')\n",
    "\n",
    "            \n",
    "    return result_df, grouped, mean_results\n",
    "    #return outputs, rmse_scores, mse_scores, true_trajectories, shot_ids, camera_matrices, distortion_coefficients, translation_vectors, rotational_matrices\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e458e",
   "metadata": {
    "papermill": {
     "duration": 0.012256,
     "end_time": "2024-05-27T09:55:55.348130",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.335874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cdc79f",
   "metadata": {
    "papermill": {
     "duration": 0.01216,
     "end_time": "2024-05-27T09:55:55.373043",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.360883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using Trajectory + Corners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b809d3",
   "metadata": {
    "papermill": {
     "duration": 0.012134,
     "end_time": "2024-05-27T09:55:55.397450",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.385316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### without spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf911d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T09:55:55.423770Z",
     "iopub.status.busy": "2024-05-27T09:55:55.423404Z",
     "iopub.status.idle": "2024-05-27T10:20:55.381672Z",
     "shell.execute_reply": "2024-05-27T10:20:55.380500Z"
    },
    "papermill": {
     "duration": 1499.974597,
     "end_time": "2024-05-27T10:20:55.384127",
     "exception": false,
     "start_time": "2024-05-27T09:55:55.409530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1558, 0.2370, 0.2118, 0.5699, 0.2445, 0.1817], device='cuda:0')\n",
      "Epoch 1/25, Train Loss: 0.0025\n",
      "Epoch 1/25, Validation Loss: 0.3014474994427449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1625, 0.2383, 0.2053, 0.5602, 0.2338, 0.1766], device='cuda:0')\n",
      "Epoch 2/25, Train Loss: 0.0016\n",
      "Epoch 2/25, Validation Loss: 0.2916487721172539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1530, 0.2319, 0.2028, 0.5484, 0.2288, 0.1797], device='cuda:0')\n",
      "Epoch 3/25, Train Loss: 0.0014\n",
      "Epoch 3/25, Validation Loss: 0.2788095973633431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1474, 0.2231, 0.1974, 0.4881, 0.2146, 0.1757], device='cuda:0')\n",
      "Epoch 4/25, Train Loss: 0.0011\n",
      "Epoch 4/25, Validation Loss: 0.20053563367676092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1352, 0.2199, 0.1932, 0.4490, 0.2076, 0.1712], device='cuda:0')\n",
      "Epoch 5/25, Train Loss: 0.0009\n",
      "Epoch 5/25, Validation Loss: 0.1935675079758103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1278, 0.2149, 0.1910, 0.4176, 0.2011, 0.1686], device='cuda:0')\n",
      "Epoch 6/25, Train Loss: 0.0009\n",
      "Epoch 6/25, Validation Loss: 0.18219285357642817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1225, 0.2103, 0.1888, 0.3948, 0.1958, 0.1661], device='cuda:0')\n",
      "Epoch 7/25, Train Loss: 0.0008\n",
      "Epoch 7/25, Validation Loss: 0.17935922016968597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1177, 0.2070, 0.1875, 0.3803, 0.1917, 0.1647], device='cuda:0')\n",
      "Epoch 8/25, Train Loss: 0.0008\n",
      "Epoch 8/25, Validation Loss: 0.18368005833110293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1133, 0.2046, 0.1862, 0.3678, 0.1893, 0.1636], device='cuda:0')\n",
      "Epoch 9/25, Train Loss: 0.0007\n",
      "Epoch 9/25, Validation Loss: 0.18145103833159884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1089, 0.2021, 0.1848, 0.3522, 0.1864, 0.1625], device='cuda:0')\n",
      "Epoch 10/25, Train Loss: 0.0007\n",
      "Epoch 10/25, Validation Loss: 0.16453899120962298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1058, 0.2001, 0.1835, 0.3406, 0.1838, 0.1613], device='cuda:0')\n",
      "Epoch 11/25, Train Loss: 0.0007\n",
      "Epoch 11/25, Validation Loss: 0.16605232857369087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1034, 0.1986, 0.1826, 0.3310, 0.1817, 0.1603], device='cuda:0')\n",
      "Epoch 12/25, Train Loss: 0.0007\n",
      "Epoch 12/25, Validation Loss: 0.16765463513297005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1013, 0.1968, 0.1814, 0.3216, 0.1795, 0.1595], device='cuda:0')\n",
      "Epoch 13/25, Train Loss: 0.0007\n",
      "Epoch 13/25, Validation Loss: 0.1608778314815985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1001, 0.1952, 0.1804, 0.3154, 0.1775, 0.1586], device='cuda:0')\n",
      "Epoch 14/25, Train Loss: 0.0007\n",
      "Epoch 14/25, Validation Loss: 0.1669247287350732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0983, 0.1939, 0.1796, 0.3084, 0.1759, 0.1582], device='cuda:0')\n",
      "Epoch 15/25, Train Loss: 0.0007\n",
      "Epoch 15/25, Validation Loss: 0.16134503242131826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0965, 0.1934, 0.1791, 0.3018, 0.1749, 0.1579], device='cuda:0')\n",
      "Epoch 16/25, Train Loss: 0.0007\n",
      "Epoch 16/25, Validation Loss: 0.16369699948542826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0948, 0.1924, 0.1786, 0.2968, 0.1736, 0.1575], device='cuda:0')\n",
      "Epoch 17/25, Train Loss: 0.0007\n",
      "Epoch 17/25, Validation Loss: 0.1629887529321619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0937, 0.1916, 0.1783, 0.2936, 0.1725, 0.1571], device='cuda:0')\n",
      "Epoch 18/25, Train Loss: 0.0007\n",
      "Epoch 18/25, Validation Loss: 0.16901002743759672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0926, 0.1909, 0.1778, 0.2884, 0.1716, 0.1568], device='cuda:0')\n",
      "Epoch 19/25, Train Loss: 0.0007\n",
      "Epoch 19/25, Validation Loss: 0.15942550732477292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0918, 0.1902, 0.1772, 0.2859, 0.1708, 0.1564], device='cuda:0')\n",
      "Epoch 20/25, Train Loss: 0.0007\n",
      "Epoch 20/25, Validation Loss: 0.1679753177874797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0909, 0.1896, 0.1768, 0.2824, 0.1699, 0.1562], device='cuda:0')\n",
      "Epoch 21/25, Train Loss: 0.0007\n",
      "Epoch 21/25, Validation Loss: 0.16182559446708575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0901, 0.1891, 0.1764, 0.2795, 0.1691, 0.1557], device='cuda:0')\n",
      "Epoch 22/25, Train Loss: 0.0007\n",
      "Epoch 22/25, Validation Loss: 0.1632284417345717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0892, 0.1886, 0.1762, 0.2759, 0.1682, 0.1555], device='cuda:0')\n",
      "Epoch 23/25, Train Loss: 0.0007\n",
      "Epoch 23/25, Validation Loss: 0.1585601064804438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0886, 0.1882, 0.1758, 0.2722, 0.1675, 0.1552], device='cuda:0')\n",
      "Epoch 24/25, Train Loss: 0.0007\n",
      "Epoch 24/25, Validation Loss: 0.15717600890108058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0878, 0.1877, 0.1756, 0.2699, 0.1672, 0.1550], device='cuda:0')\n",
      "Epoch 25/25, Train Loss: 0.0007\n",
      "Epoch 25/25, Validation Loss: 0.16356517817523028\n",
      "time it took to train : 1406.394695520401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game\n",
      "game1    [0.4539912748597565, 2.1704513208998417, 0.500...\n",
      "game8    [0.4153808744703075, 2.4193155211444615, 0.529...\n",
      "Name: Start Error, dtype: object\n",
      "could find mean\n",
      "time it took to test on synthetic : 63.792314529418945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took to test on real true : 15.918471574783325\n",
      "time it took to test on real predicted : 13.765521764755249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "path = 'with_corners/without_spin/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "start = time()\n",
    "model, loss = train_syn(num_epochs = 25,\n",
    "                        model_ = CustomModel,\n",
    "                        lr = 0.005,\n",
    "                        use_court = True, \n",
    "                        use_cam_params = False,\n",
    "                        use_positions = False,\n",
    "                        spin = False)\n",
    "\n",
    "print(f'time it took to train : {time()-start}')\n",
    "## SAVE MODEL\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(path,'model.pth'))\n",
    "\n",
    "# Synthetic\n",
    "start = time()\n",
    "df_syn, grouped_syn, mean_results_syn = predict_NN(data_loader = syn_loader_val_NN, \n",
    "                                                   model = model,\n",
    "                                                   spin = False,\n",
    "                                                   use_court = True, \n",
    "                                                   use_cam_params = False, \n",
    "                                                   use_positions = False, \n",
    "                                                   synthetic_data = True)\n",
    "\n",
    "df_syn.to_csv(os.path.join(path, 'all_shots_synthetic.csv'))\n",
    "grouped_syn.to_csv(os.path.join(path, 'by_game_synthetic.csv'))\n",
    "with open(os.path.join(path,\"mean_results_synthetic.json\"), 'w') as f:\n",
    "    json.dump(mean_results_syn, f)\n",
    "    \n",
    "print(f'time it took to test on synthetic : {time()-start}')\n",
    "\n",
    "# true all\n",
    "start = time()\n",
    "df_true, grouped_true, mean_results_true = predict_NN(data_loader = real_all_loader_NN_true, \n",
    "                                                      model = model,\n",
    "                                                      spin=False,\n",
    "                                                      use_court = True, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "\n",
    "df_true.to_csv(os.path.join(path, 'all_shots_true.csv'))\n",
    "grouped_true.to_csv(os.path.join(path, 'by_game_true.csv'))\n",
    "with open(os.path.join(path,\"mean_results_true.json\"), 'w') as f:\n",
    "    json.dump(mean_results_true, f)\n",
    "\n",
    "print(f'time it took to test on real true : {time()-start}')\n",
    "    \n",
    "    \n",
    "# predicted all\n",
    "start=time()\n",
    "df_pred, grouped_pred, mean_results_pred = predict_NN(data_loader = real_all_loader_NN_pred, \n",
    "                                                      model = model,\n",
    "                                                      spin=False,\n",
    "                                                      use_court = True, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "print(f'time it took to test on real predicted : {time()-start}')\n",
    "\n",
    "df_pred.to_csv(os.path.join(path, 'all_shots_predicted.csv'))\n",
    "grouped_pred.to_csv(os.path.join(path, 'by_game_predicted.csv'))\n",
    "with open(os.path.join(path,\"mean_results_predicted.json\"), 'w') as f:\n",
    "    json.dump(mean_results_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0425b",
   "metadata": {
    "papermill": {
     "duration": 0.324553,
     "end_time": "2024-05-27T10:20:56.037377",
     "exception": false,
     "start_time": "2024-05-27T10:20:55.712824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### with spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a737517a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T10:20:56.686757Z",
     "iopub.status.busy": "2024-05-27T10:20:56.686220Z",
     "iopub.status.idle": "2024-05-27T10:45:52.100080Z",
     "shell.execute_reply": "2024-05-27T10:45:52.099152Z"
    },
    "papermill": {
     "duration": 1496.386483,
     "end_time": "2024-05-27T10:45:52.746414",
     "exception": false,
     "start_time": "2024-05-27T10:20:56.359931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3099, 0.3725, 0.2529, 0.5791, 0.4411, 0.2547, 0.4700, 0.5788, 0.5768],\n",
      "       device='cuda:0')\n",
      "Epoch 1/25, Train Loss: 0.0037\n",
      "Epoch 1/25, Validation Loss: 0.44640617435042923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2262, 0.3421, 0.2437, 0.5724, 0.3884, 0.2545, 0.4529, 0.5750, 0.5771],\n",
      "       device='cuda:0')\n",
      "Epoch 2/25, Train Loss: 0.0028\n",
      "Epoch 2/25, Validation Loss: 0.41195981566970413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1981, 0.3032, 0.2382, 0.5688, 0.3361, 0.2531, 0.4429, 0.5745, 0.5752],\n",
      "       device='cuda:0')\n",
      "Epoch 3/25, Train Loss: 0.0026\n",
      "Epoch 3/25, Validation Loss: 0.3936922228014147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1785, 0.2847, 0.2291, 0.5639, 0.3097, 0.2541, 0.4292, 0.5737, 0.5743],\n",
      "       device='cuda:0')\n",
      "Epoch 4/25, Train Loss: 0.0025\n",
      "Epoch 4/25, Validation Loss: 0.38542883460586136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1817, 0.2693, 0.2236, 0.5571, 0.2861, 0.2524, 0.4196, 0.5731, 0.5727],\n",
      "       device='cuda:0')\n",
      "Epoch 5/25, Train Loss: 0.0024\n",
      "Epoch 5/25, Validation Loss: 0.37966141587979085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1686, 0.2588, 0.2191, 0.5361, 0.2698, 0.2523, 0.4124, 0.5723, 0.5712],\n",
      "       device='cuda:0')\n",
      "Epoch 6/25, Train Loss: 0.0024\n",
      "Epoch 6/25, Validation Loss: 0.3594687307203138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1578, 0.2497, 0.2147, 0.5098, 0.2541, 0.2516, 0.4047, 0.5718, 0.5696],\n",
      "       device='cuda:0')\n",
      "Epoch 7/25, Train Loss: 0.0022\n",
      "Epoch 7/25, Validation Loss: 0.3436510587060774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1490, 0.2431, 0.2117, 0.4878, 0.2424, 0.2506, 0.3992, 0.5713, 0.5687],\n",
      "       device='cuda:0')\n",
      "Epoch 8/25, Train Loss: 0.0022\n",
      "Epoch 8/25, Validation Loss: 0.34209547574455673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1416, 0.2374, 0.2088, 0.4684, 0.2328, 0.2495, 0.3943, 0.5712, 0.5678],\n",
      "       device='cuda:0')\n",
      "Epoch 9/25, Train Loss: 0.0021\n",
      "Epoch 9/25, Validation Loss: 0.33761567441192836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1367, 0.2329, 0.2058, 0.4569, 0.2248, 0.2484, 0.3903, 0.5709, 0.5668],\n",
      "       device='cuda:0')\n",
      "Epoch 10/25, Train Loss: 0.0021\n",
      "Epoch 10/25, Validation Loss: 0.3411956397262779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1316, 0.2293, 0.2036, 0.4446, 0.2187, 0.2475, 0.3872, 0.5708, 0.5661],\n",
      "       device='cuda:0')\n",
      "Epoch 11/25, Train Loss: 0.0021\n",
      "Epoch 11/25, Validation Loss: 0.33840149077209264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1274, 0.2259, 0.2016, 0.4338, 0.2132, 0.2466, 0.3843, 0.5704, 0.5656],\n",
      "       device='cuda:0')\n",
      "Epoch 12/25, Train Loss: 0.0021\n",
      "Epoch 12/25, Validation Loss: 0.3358777076811404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1239, 0.2230, 0.2001, 0.4244, 0.2085, 0.2458, 0.3818, 0.5702, 0.5650],\n",
      "       device='cuda:0')\n",
      "Epoch 13/25, Train Loss: 0.0021\n",
      "Epoch 13/25, Validation Loss: 0.3354973124491202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1203, 0.2207, 0.1989, 0.4164, 0.2043, 0.2452, 0.3797, 0.5698, 0.5648],\n",
      "       device='cuda:0')\n",
      "Epoch 14/25, Train Loss: 0.0021\n",
      "Epoch 14/25, Validation Loss: 0.3359935565574749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1176, 0.2188, 0.1976, 0.4085, 0.2009, 0.2445, 0.3778, 0.5695, 0.5645],\n",
      "       device='cuda:0')\n",
      "Epoch 15/25, Train Loss: 0.0020\n",
      "Epoch 15/25, Validation Loss: 0.33419754537376195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1151, 0.2170, 0.1964, 0.4014, 0.1979, 0.2440, 0.3762, 0.5694, 0.5641],\n",
      "       device='cuda:0')\n",
      "Epoch 16/25, Train Loss: 0.0020\n",
      "Epoch 16/25, Validation Loss: 0.33367132012908524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1130, 0.2157, 0.1956, 0.3948, 0.1953, 0.2436, 0.3747, 0.5693, 0.5638],\n",
      "       device='cuda:0')\n",
      "Epoch 17/25, Train Loss: 0.0020\n",
      "Epoch 17/25, Validation Loss: 0.33369803589743535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1110, 0.2142, 0.1948, 0.3894, 0.1927, 0.2431, 0.3735, 0.5692, 0.5635],\n",
      "       device='cuda:0')\n",
      "Epoch 18/25, Train Loss: 0.0020\n",
      "Epoch 18/25, Validation Loss: 0.3334884989906002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1094, 0.2129, 0.1940, 0.3842, 0.1904, 0.2427, 0.3723, 0.5692, 0.5631],\n",
      "       device='cuda:0')\n",
      "Epoch 19/25, Train Loss: 0.0020\n",
      "Epoch 19/25, Validation Loss: 0.33319154542845647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1078, 0.2117, 0.1932, 0.3801, 0.1884, 0.2422, 0.3713, 0.5691, 0.5630],\n",
      "       device='cuda:0')\n",
      "Epoch 20/25, Train Loss: 0.0020\n",
      "Epoch 20/25, Validation Loss: 0.3346016012333535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1065, 0.2107, 0.1926, 0.3764, 0.1866, 0.2419, 0.3703, 0.5692, 0.5627],\n",
      "       device='cuda:0')\n",
      "Epoch 21/25, Train Loss: 0.0020\n",
      "Epoch 21/25, Validation Loss: 0.33453358347351486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1052, 0.2098, 0.1921, 0.3728, 0.1851, 0.2416, 0.3695, 0.5691, 0.5626],\n",
      "       device='cuda:0')\n",
      "Epoch 22/25, Train Loss: 0.0020\n",
      "Epoch 22/25, Validation Loss: 0.33436174892090464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1038, 0.2090, 0.1916, 0.3690, 0.1837, 0.2413, 0.3687, 0.5690, 0.5623],\n",
      "       device='cuda:0')\n",
      "Epoch 23/25, Train Loss: 0.0020\n",
      "Epoch 23/25, Validation Loss: 0.332288598692095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1028, 0.2084, 0.1913, 0.3655, 0.1826, 0.2410, 0.3679, 0.5689, 0.5623],\n",
      "       device='cuda:0')\n",
      "Epoch 24/25, Train Loss: 0.0020\n",
      "Epoch 24/25, Validation Loss: 0.33392968451654587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1017, 0.2077, 0.1908, 0.3631, 0.1813, 0.2407, 0.3672, 0.5690, 0.5621],\n",
      "       device='cuda:0')\n",
      "Epoch 25/25, Train Loss: 0.0020\n",
      "Epoch 25/25, Validation Loss: 0.334424904069385\n",
      "time it took to train : 1390.5164813995361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game\n",
      "game1    [0.4827139038037658, 2.4060753548425877, 0.530...\n",
      "game8    [0.48753793962961867, 2.589915422113301, 0.553...\n",
      "Name: Start Error, dtype: object\n",
      "could find mean\n",
      "time it took to test on synthetic : 73.27429008483887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took to test on real true : 16.176509857177734\n",
      "time it took to test on real predicted : 14.952239513397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "path = 'with_corners/with_spin/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "start = time()\n",
    "model, loss = train_syn(num_epochs = 25,\n",
    "                        model_ = CustomModel,\n",
    "                        lr = 0.005,\n",
    "                        use_court = True, \n",
    "                        use_cam_params = False,\n",
    "                        use_positions = False,\n",
    "                        spin = True)\n",
    "\n",
    "\n",
    "print(f'time it took to train : {time()-start}')\n",
    "## SAVE MODEL\n",
    "torch.save(model.state_dict(), os.path.join(path,'model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# Synthetic\n",
    "start = time()\n",
    "df_syn, grouped_syn, mean_results_syn = predict_NN(data_loader = syn_loader_val_NN_spin, \n",
    "                                                   model = model,\n",
    "                                                   spin = True,\n",
    "                                                   use_court = True, \n",
    "                                                   use_cam_params = False, \n",
    "                                                   use_positions = False, \n",
    "                                                   synthetic_data = True)\n",
    "\n",
    "print(f'time it took to test on synthetic : {time()-start}')\n",
    "df_syn.to_csv(os.path.join(path, 'all_shots_synthetic.csv'))\n",
    "grouped_syn.to_csv(os.path.join(path, 'by_game_synthetic.csv'))\n",
    "with open(os.path.join(path,\"mean_results_synthetic.json\"), 'w') as f:\n",
    "    json.dump(mean_results_syn, f)\n",
    "\n",
    "# true all\n",
    "start = time()\n",
    "df_true, grouped_true, mean_results_true = predict_NN(data_loader = real_all_loader_NN_true, \n",
    "                                                      model = model,\n",
    "                                                      spin=True,\n",
    "                                                      use_court = True, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "\n",
    "print(f'time it took to test on real true : {time()-start}')\n",
    "df_true.to_csv(os.path.join(path, 'all_shots_true.csv'))\n",
    "grouped_true.to_csv(os.path.join(path, 'by_game_true.csv'))\n",
    "with open(os.path.join(path,\"mean_results_true.json\"), 'w') as f:\n",
    "    json.dump(mean_results_true, f)\n",
    "\n",
    "# predicted all\n",
    "start = time()\n",
    "df_pred, grouped_pred, mean_results_pred = predict_NN(data_loader = real_all_loader_NN_pred, \n",
    "                                                      model = model,\n",
    "                                                      spin=True,\n",
    "                                                      use_court = True, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "print(f'time it took to test on real predicted : {time()-start}')\n",
    "\n",
    "df_pred.to_csv(os.path.join(path, 'all_shots_predicted.csv'))\n",
    "grouped_pred.to_csv(os.path.join(path, 'by_game_predicted.csv'))\n",
    "with open(os.path.join(path,\"mean_results_predicted.json\"), 'w') as f:\n",
    "    json.dump(mean_results_pred, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09569111",
   "metadata": {
    "papermill": {
     "duration": 0.642829,
     "end_time": "2024-05-27T10:45:54.090327",
     "exception": false,
     "start_time": "2024-05-27T10:45:53.447498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Without Corners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f153eb",
   "metadata": {
    "papermill": {
     "duration": 0.707724,
     "end_time": "2024-05-27T10:45:55.441609",
     "exception": false,
     "start_time": "2024-05-27T10:45:54.733885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### without spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb7e69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T10:45:56.757665Z",
     "iopub.status.busy": "2024-05-27T10:45:56.757273Z",
     "iopub.status.idle": "2024-05-27T11:10:15.960180Z",
     "shell.execute_reply": "2024-05-27T11:10:15.958940Z"
    },
    "papermill": {
     "duration": 1459.88027,
     "end_time": "2024-05-27T11:10:15.962522",
     "exception": false,
     "start_time": "2024-05-27T10:45:56.082252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1598, 0.2689, 0.2193, 0.5496, 0.2440, 0.1741], device='cuda:0')\n",
      "Epoch 1/25, Train Loss: 0.0023\n",
      "Epoch 1/25, Validation Loss: 0.30036658894371343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1668, 0.2510, 0.2136, 0.5426, 0.2256, 0.1800], device='cuda:0')\n",
      "Epoch 2/25, Train Loss: 0.0016\n",
      "Epoch 2/25, Validation Loss: 0.2871890478842967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1467, 0.2418, 0.2069, 0.4722, 0.2164, 0.1754], device='cuda:0')\n",
      "Epoch 3/25, Train Loss: 0.0012\n",
      "Epoch 3/25, Validation Loss: 0.21539394919936722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1307, 0.2356, 0.2026, 0.4283, 0.2076, 0.1720], device='cuda:0')\n",
      "Epoch 4/25, Train Loss: 0.0010\n",
      "Epoch 4/25, Validation Loss: 0.19987461776346774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1226, 0.2317, 0.1989, 0.3984, 0.2004, 0.1693], device='cuda:0')\n",
      "Epoch 5/25, Train Loss: 0.0009\n",
      "Epoch 5/25, Validation Loss: 0.1932396244358372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1201, 0.2282, 0.1968, 0.3748, 0.1941, 0.1685], device='cuda:0')\n",
      "Epoch 6/25, Train Loss: 0.0009\n",
      "Epoch 6/25, Validation Loss: 0.18790902479274854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1144, 0.2248, 0.1952, 0.3575, 0.1890, 0.1676], device='cuda:0')\n",
      "Epoch 7/25, Train Loss: 0.0008\n",
      "Epoch 7/25, Validation Loss: 0.18241625460418495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1103, 0.2242, 0.1940, 0.3466, 0.1858, 0.1677], device='cuda:0')\n",
      "Epoch 8/25, Train Loss: 0.0008\n",
      "Epoch 8/25, Validation Loss: 0.1914957979240933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1066, 0.2225, 0.1930, 0.3332, 0.1831, 0.1669], device='cuda:0')\n",
      "Epoch 9/25, Train Loss: 0.0008\n",
      "Epoch 9/25, Validation Loss: 0.17749625323592005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1037, 0.2214, 0.1918, 0.3219, 0.1807, 0.1659], device='cuda:0')\n",
      "Epoch 10/25, Train Loss: 0.0008\n",
      "Epoch 10/25, Validation Loss: 0.1749813745956163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1025, 0.2197, 0.1907, 0.3131, 0.1784, 0.1650], device='cuda:0')\n",
      "Epoch 11/25, Train Loss: 0.0008\n",
      "Epoch 11/25, Validation Loss: 0.1744744052758088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1007, 0.2187, 0.1901, 0.3057, 0.1769, 0.1647], device='cuda:0')\n",
      "Epoch 12/25, Train Loss: 0.0008\n",
      "Epoch 12/25, Validation Loss: 0.1765736208574192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0993, 0.2174, 0.1894, 0.3005, 0.1757, 0.1643], device='cuda:0')\n",
      "Epoch 13/25, Train Loss: 0.0007\n",
      "Epoch 13/25, Validation Loss: 0.17786412464605794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0971, 0.2168, 0.1890, 0.2936, 0.1744, 0.1636], device='cuda:0')\n",
      "Epoch 14/25, Train Loss: 0.0007\n",
      "Epoch 14/25, Validation Loss: 0.17069718120871363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0951, 0.2158, 0.1885, 0.2879, 0.1734, 0.1631], device='cuda:0')\n",
      "Epoch 15/25, Train Loss: 0.0007\n",
      "Epoch 15/25, Validation Loss: 0.16988084364581751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0943, 0.2150, 0.1881, 0.2829, 0.1725, 0.1629], device='cuda:0')\n",
      "Epoch 16/25, Train Loss: 0.0007\n",
      "Epoch 16/25, Validation Loss: 0.17160264946318962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0941, 0.2142, 0.1874, 0.2804, 0.1716, 0.1626], device='cuda:0')\n",
      "Epoch 17/25, Train Loss: 0.0007\n",
      "Epoch 17/25, Validation Loss: 0.1777216385345201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0934, 0.2137, 0.1870, 0.2762, 0.1707, 0.1621], device='cuda:0')\n",
      "Epoch 18/25, Train Loss: 0.0007\n",
      "Epoch 18/25, Validation Loss: 0.16996999568230398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0923, 0.2129, 0.1868, 0.2728, 0.1704, 0.1618], device='cuda:0')\n",
      "Epoch 19/25, Train Loss: 0.0007\n",
      "Epoch 19/25, Validation Loss: 0.17144398995347926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0911, 0.2122, 0.1864, 0.2689, 0.1701, 0.1616], device='cuda:0')\n",
      "Epoch 20/25, Train Loss: 0.0007\n",
      "Epoch 20/25, Validation Loss: 0.16712252554055806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0906, 0.2123, 0.1863, 0.2664, 0.1695, 0.1614], device='cuda:0')\n",
      "Epoch 21/25, Train Loss: 0.0007\n",
      "Epoch 21/25, Validation Loss: 0.1757498725846007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0901, 0.2117, 0.1861, 0.2641, 0.1690, 0.1611], device='cuda:0')\n",
      "Epoch 22/25, Train Loss: 0.0007\n",
      "Epoch 22/25, Validation Loss: 0.17052670910551743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0898, 0.2112, 0.1859, 0.2616, 0.1686, 0.1609], device='cuda:0')\n",
      "Epoch 23/25, Train Loss: 0.0007\n",
      "Epoch 23/25, Validation Loss: 0.17077231165525075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0895, 0.2107, 0.1857, 0.2597, 0.1683, 0.1606], device='cuda:0')\n",
      "Epoch 24/25, Train Loss: 0.0007\n",
      "Epoch 24/25, Validation Loss: 0.1720548107011898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0889, 0.2105, 0.1855, 0.2590, 0.1678, 0.1604], device='cuda:0')\n",
      "Epoch 25/25, Train Loss: 0.0007\n",
      "Epoch 25/25, Validation Loss: 0.17796073813696164\n",
      "time it took to train : 1367.9196956157684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game\n",
      "game1    [0.49096329907041786, 2.4733259047882044, 0.52...\n",
      "game8    [0.4765252517869483, 2.9017350342514616, 0.576...\n",
      "Name: Start Error, dtype: object\n",
      "could find mean\n",
      "time it took to test on synthetic : 62.62806177139282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took to test on real true : 14.307600975036621\n",
      "time it took to test on real predicted : 13.88283896446228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "path = 'without_corners/without_spin/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "start = time()\n",
    "model, loss = train_syn(num_epochs = 25,\n",
    "                        model_ = CustomModel,\n",
    "                        lr = 0.005,\n",
    "                        use_court = False, \n",
    "                        use_cam_params = False,\n",
    "                        use_positions = False,\n",
    "                        spin = False)\n",
    "print(f'time it took to train : {time()-start}')\n",
    "## SAVE MODEL\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(path,'model.pth'))\n",
    "\n",
    "# Synthetic\n",
    "start = time()\n",
    "df_syn, grouped_syn, mean_results_syn = predict_NN(data_loader = syn_loader_val_NN, \n",
    "                                                   model = model,\n",
    "                                                   spin = False,\n",
    "                                                   use_court = False, \n",
    "                                                   use_cam_params = False, \n",
    "                                                   use_positions = False, \n",
    "                                                   synthetic_data = True)\n",
    "print(f'time it took to test on synthetic : {time()-start}')\n",
    "df_syn.to_csv(os.path.join(path, 'all_shots_synthetic.csv'))\n",
    "grouped_syn.to_csv(os.path.join(path, 'by_game_synthetic.csv'))\n",
    "with open(os.path.join(path,\"mean_results_synthetic.json\"), 'w') as f:\n",
    "    json.dump(mean_results_syn, f)\n",
    "\n",
    "# true all\n",
    "start = time()\n",
    "df_true, grouped_true, mean_results_true = predict_NN(data_loader = real_all_loader_NN_true, \n",
    "                                                      model = model,\n",
    "                                                      spin=False,\n",
    "                                                      use_court = False, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "\n",
    "print(f'time it took to test on real true : {time()-start}')\n",
    "\n",
    "df_true.to_csv(os.path.join(path, 'all_shots_true.csv'))\n",
    "grouped_true.to_csv(os.path.join(path, 'by_game_true.csv'))\n",
    "with open(os.path.join(path,\"mean_results_true.json\"), 'w') as f:\n",
    "    json.dump(mean_results_true, f)\n",
    "    \n",
    "    \n",
    "# predicted all\n",
    "start = time()\n",
    "df_pred, grouped_pred, mean_results_pred = predict_NN(data_loader = real_all_loader_NN_pred, \n",
    "                                                      model = model,\n",
    "                                                      spin=False,\n",
    "                                                      use_court = False, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "\n",
    "print(f'time it took to test on real predicted : {time()-start}')\n",
    "df_pred.to_csv(os.path.join(path, 'all_shots_predicted.csv'))\n",
    "grouped_pred.to_csv(os.path.join(path, 'by_game_predicted.csv'))\n",
    "with open(os.path.join(path,\"mean_results_predicted.json\"), 'w') as f:\n",
    "    json.dump(mean_results_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b671288",
   "metadata": {
    "papermill": {
     "duration": 1.020759,
     "end_time": "2024-05-27T11:10:17.950510",
     "exception": false,
     "start_time": "2024-05-27T11:10:16.929751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### with spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3a00f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:10:19.953055Z",
     "iopub.status.busy": "2024-05-27T11:10:19.952679Z",
     "iopub.status.idle": "2024-05-27T11:35:24.274229Z",
     "shell.execute_reply": "2024-05-27T11:35:24.273177Z"
    },
    "papermill": {
     "duration": 1505.341564,
     "end_time": "2024-05-27T11:35:24.276970",
     "exception": false,
     "start_time": "2024-05-27T11:10:18.935406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1628, 0.4496, 0.2465, 0.5796, 0.5348, 0.2706, 0.5320, 0.5718, 0.5834],\n",
      "       device='cuda:0')\n",
      "Epoch 1/25, Train Loss: 0.0037\n",
      "Epoch 1/25, Validation Loss: 0.46478922302658493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1554, 0.3640, 0.2439, 0.5731, 0.4242, 0.2628, 0.4929, 0.5768, 0.5770],\n",
      "       device='cuda:0')\n",
      "Epoch 2/25, Train Loss: 0.0028\n",
      "Epoch 2/25, Validation Loss: 0.4108899155178586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1427, 0.3190, 0.2358, 0.5669, 0.3607, 0.2578, 0.4705, 0.5743, 0.5744],\n",
      "       device='cuda:0')\n",
      "Epoch 3/25, Train Loss: 0.0026\n",
      "Epoch 3/25, Validation Loss: 0.39098481793661377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1395, 0.2922, 0.2325, 0.5630, 0.3163, 0.2546, 0.4540, 0.5734, 0.5731],\n",
      "       device='cuda:0')\n",
      "Epoch 4/25, Train Loss: 0.0025\n",
      "Epoch 4/25, Validation Loss: 0.38399914873612895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1367, 0.2765, 0.2315, 0.5582, 0.2886, 0.2520, 0.4435, 0.5728, 0.5719],\n",
      "       device='cuda:0')\n",
      "Epoch 5/25, Train Loss: 0.0025\n",
      "Epoch 5/25, Validation Loss: 0.38096211327088847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1357, 0.2668, 0.2298, 0.5483, 0.2689, 0.2512, 0.4350, 0.5722, 0.5707],\n",
      "       device='cuda:0')\n",
      "Epoch 6/25, Train Loss: 0.0024\n",
      "Epoch 6/25, Validation Loss: 0.3731845158177453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1326, 0.2592, 0.2277, 0.5269, 0.2541, 0.2510, 0.4296, 0.5719, 0.5696],\n",
      "       device='cuda:0')\n",
      "Epoch 7/25, Train Loss: 0.0023\n",
      "Epoch 7/25, Validation Loss: 0.35882634085577886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1290, 0.2531, 0.2273, 0.5094, 0.2433, 0.2503, 0.4256, 0.5717, 0.5686],\n",
      "       device='cuda:0')\n",
      "Epoch 8/25, Train Loss: 0.0022\n",
      "Epoch 8/25, Validation Loss: 0.3568008332639127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1255, 0.2479, 0.2258, 0.4922, 0.2340, 0.2499, 0.4215, 0.5713, 0.5675],\n",
      "       device='cuda:0')\n",
      "Epoch 9/25, Train Loss: 0.0022\n",
      "Epoch 9/25, Validation Loss: 0.34978780955881683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1220, 0.2440, 0.2246, 0.4755, 0.2265, 0.2491, 0.4181, 0.5710, 0.5667],\n",
      "       device='cuda:0')\n",
      "Epoch 10/25, Train Loss: 0.0022\n",
      "Epoch 10/25, Validation Loss: 0.34609252375525396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1190, 0.2405, 0.2239, 0.4615, 0.2205, 0.2485, 0.4155, 0.5709, 0.5659],\n",
      "       device='cuda:0')\n",
      "Epoch 11/25, Train Loss: 0.0022\n",
      "Epoch 11/25, Validation Loss: 0.3459375395968154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1162, 0.2377, 0.2235, 0.4494, 0.2153, 0.2480, 0.4133, 0.5708, 0.5655],\n",
      "       device='cuda:0')\n",
      "Epoch 12/25, Train Loss: 0.0022\n",
      "Epoch 12/25, Validation Loss: 0.34547676186303833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1137, 0.2353, 0.2227, 0.4385, 0.2114, 0.2473, 0.4113, 0.5706, 0.5651],\n",
      "       device='cuda:0')\n",
      "Epoch 13/25, Train Loss: 0.0021\n",
      "Epoch 13/25, Validation Loss: 0.34410016117869197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1115, 0.2332, 0.2222, 0.4296, 0.2078, 0.2469, 0.4097, 0.5705, 0.5647],\n",
      "       device='cuda:0')\n",
      "Epoch 14/25, Train Loss: 0.0021\n",
      "Epoch 14/25, Validation Loss: 0.34512305984625946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1098, 0.2314, 0.2219, 0.4217, 0.2046, 0.2465, 0.4081, 0.5703, 0.5643],\n",
      "       device='cuda:0')\n",
      "Epoch 15/25, Train Loss: 0.0021\n",
      "Epoch 15/25, Validation Loss: 0.3441332812244828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1081, 0.2297, 0.2215, 0.4143, 0.2018, 0.2461, 0.4066, 0.5701, 0.5640],\n",
      "       device='cuda:0')\n",
      "Epoch 16/25, Train Loss: 0.0021\n",
      "Epoch 16/25, Validation Loss: 0.3428441141102765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1065, 0.2283, 0.2211, 0.4077, 0.1996, 0.2459, 0.4057, 0.5699, 0.5638],\n",
      "       device='cuda:0')\n",
      "Epoch 17/25, Train Loss: 0.0021\n",
      "Epoch 17/25, Validation Loss: 0.34384979267378113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1051, 0.2270, 0.2208, 0.4024, 0.1973, 0.2455, 0.4046, 0.5698, 0.5635],\n",
      "       device='cuda:0')\n",
      "Epoch 18/25, Train Loss: 0.0021\n",
      "Epoch 18/25, Validation Loss: 0.34392491707930695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1041, 0.2259, 0.2206, 0.3972, 0.1955, 0.2452, 0.4039, 0.5697, 0.5632],\n",
      "       device='cuda:0')\n",
      "Epoch 19/25, Train Loss: 0.0021\n",
      "Epoch 19/25, Validation Loss: 0.34350172249046534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1029, 0.2251, 0.2205, 0.3925, 0.1936, 0.2449, 0.4030, 0.5696, 0.5630],\n",
      "       device='cuda:0')\n",
      "Epoch 20/25, Train Loss: 0.0021\n",
      "Epoch 20/25, Validation Loss: 0.3434254736513705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1019, 0.2242, 0.2202, 0.3882, 0.1922, 0.2447, 0.4023, 0.5695, 0.5628],\n",
      "       device='cuda:0')\n",
      "Epoch 21/25, Train Loss: 0.0021\n",
      "Epoch 21/25, Validation Loss: 0.34309370292199626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1010, 0.2234, 0.2199, 0.3847, 0.1906, 0.2445, 0.4015, 0.5695, 0.5628],\n",
      "       device='cuda:0')\n",
      "Epoch 22/25, Train Loss: 0.0021\n",
      "Epoch 22/25, Validation Loss: 0.3441159926556252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1002, 0.2226, 0.2197, 0.3810, 0.1892, 0.2443, 0.4008, 0.5694, 0.5627],\n",
      "       device='cuda:0')\n",
      "Epoch 23/25, Train Loss: 0.0021\n",
      "Epoch 23/25, Validation Loss: 0.34303052199853434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0992, 0.2219, 0.2194, 0.3778, 0.1879, 0.2442, 0.4003, 0.5693, 0.5626],\n",
      "       device='cuda:0')\n",
      "Epoch 24/25, Train Loss: 0.0021\n",
      "Epoch 24/25, Validation Loss: 0.34293890402123733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0984, 0.2213, 0.2192, 0.3746, 0.1868, 0.2440, 0.3997, 0.5692, 0.5625],\n",
      "       device='cuda:0')\n",
      "Epoch 25/25, Train Loss: 0.0021\n",
      "Epoch 25/25, Validation Loss: 0.3424751951887801\n",
      "time it took to train : 1400.0312411785126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df.groupby('Game').apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game\n",
      "game1    [0.5047751067904196, 2.567980616027132, 0.6003...\n",
      "game8    [0.5163913466581813, 2.7886746255841186, 0.704...\n",
      "Name: Start Error, dtype: object\n",
      "could find mean\n",
      "time it took to test on synthetic : 73.0206732749939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took to test on real true : 15.670043706893921\n",
      "time it took to test on real predicted : 30.85152530670166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1157814838.py:170: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = result_df[[\"Game\",\"MSE\", \"RMSE\", \"predicted_tiles\", \"true_tiles\", 'landing_error']].groupby(\"Game\").apply(lambda group: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "path = 'without_corners/with_spin/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "start = time()\n",
    "model, loss = train_syn(num_epochs = 25,\n",
    "                        model_ = CustomModel,\n",
    "                        lr = 0.005,\n",
    "                        use_court = False, \n",
    "                        use_cam_params = False,\n",
    "                        use_positions = False,\n",
    "                        spin = True)\n",
    "print(f'time it took to train : {time()-start}')\n",
    "## SAVE MODEL\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(path,'model.pth'))\n",
    "\n",
    "# Synthetic\n",
    "start = time()\n",
    "df_syn, grouped_syn, mean_results_syn = predict_NN(data_loader = syn_loader_val_NN_spin, \n",
    "                                                   model = model,\n",
    "                                                   spin = True,\n",
    "                                                   use_court = False, \n",
    "                                                   use_cam_params = False, \n",
    "                                                   use_positions = False, \n",
    "                                                   synthetic_data = True)\n",
    "print(f'time it took to test on synthetic : {time()-start}')\n",
    "df_syn.to_csv(os.path.join(path, 'all_shots_synthetic.csv'))\n",
    "grouped_syn.to_csv(os.path.join(path, 'by_game_synthetic.csv'))\n",
    "with open(os.path.join(path,\"mean_results_synthetic.json\"), 'w') as f:\n",
    "    json.dump(mean_results_syn, f)\n",
    "\n",
    "# true all\n",
    "start = time()\n",
    "df_true, grouped_true, mean_results_true = predict_NN(data_loader = real_all_loader_NN_true, \n",
    "                                                      model = model,\n",
    "                                                      spin=True,\n",
    "                                                      use_court = False, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "print(f'time it took to test on real true : {time()-start}')\n",
    "df_true.to_csv(os.path.join(path, 'all_shots_true.csv'))\n",
    "grouped_true.to_csv(os.path.join(path, 'by_game_true.csv'))\n",
    "with open(os.path.join(path,\"mean_results_true.json\"), 'w') as f:\n",
    "    json.dump(mean_results_true, f)\n",
    "    \n",
    "    \n",
    "# predicted all\n",
    "sttart = time()\n",
    "df_pred, grouped_pred, mean_results_pred = predict_NN(data_loader = real_all_loader_NN_pred, \n",
    "                                                      model = model,\n",
    "                                                      spin=True,\n",
    "                                                      use_court = False, \n",
    "                                                      use_cam_params = False, \n",
    "                                                      use_positions = False, \n",
    "                                                      synthetic_data = False)\n",
    "\n",
    "print(f'time it took to test on real predicted : {time()-start}')\n",
    "df_pred.to_csv(os.path.join(path, 'all_shots_predicted.csv'))\n",
    "grouped_pred.to_csv(os.path.join(path, 'by_game_predicted.csv'))\n",
    "with open(os.path.join(path,\"mean_results_predicted.json\"), 'w') as f:\n",
    "    json.dump(mean_results_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5052b84",
   "metadata": {
    "papermill": {
     "duration": 1.369651,
     "end_time": "2024-05-27T11:35:26.939615",
     "exception": false,
     "start_time": "2024-05-27T11:35:25.569964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b6a49d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:35:29.633735Z",
     "iopub.status.busy": "2024-05-27T11:35:29.633356Z",
     "iopub.status.idle": "2024-05-27T11:35:29.643583Z",
     "shell.execute_reply": "2024-05-27T11:35:29.642898Z"
    },
    "papermill": {
     "duration": 1.413352,
     "end_time": "2024-05-27T11:35:29.645523",
     "exception": false,
     "start_time": "2024-05-27T11:35:28.232171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/kaggle/working/TennisTrajectoryReconstruction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f365527",
   "metadata": {
    "papermill": {
     "duration": 1.307697,
     "end_time": "2024-05-27T11:35:32.313531",
     "exception": false,
     "start_time": "2024-05-27T11:35:31.005834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e78e419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:35:35.008891Z",
     "iopub.status.busy": "2024-05-27T11:35:35.008486Z",
     "iopub.status.idle": "2024-05-27T11:35:35.017665Z",
     "shell.execute_reply": "2024-05-27T11:35:35.016756Z"
    },
    "papermill": {
     "duration": 1.320268,
     "end_time": "2024-05-27T11:35:35.019517",
     "exception": false,
     "start_time": "2024-05-27T11:35:33.699249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def batch_loss(output, mtx, dist, rvecs, tvecs, rotation_matrix, inputs):\n",
    "    \n",
    "\n",
    "        \n",
    "    loss = 0\n",
    "    \n",
    "\n",
    "    batch = output.shape[0]\n",
    "    \n",
    "    for i in range(batch):\n",
    "        curr_input = inputs[i]\n",
    "        \n",
    "        mask = curr_input != -1\n",
    "        non_padded_input = curr_input[mask].view(-1,2)\n",
    "        \n",
    "        N = non_padded_input.shape[0]\n",
    "\n",
    "        mse_loss = nn.MSELoss()\n",
    "\n",
    "        traj = create_3d_trajectory(output[i].view(1, 6), N)\n",
    "\n",
    "        points_2d = project_points_torch(traj, rotation_matrix[i], tvecs[i], mtx[i],dist[i])\n",
    "        \n",
    "        points_2d[:, 0] = points_2d[:,0] / 1280\n",
    "        points_2d[:, 1] = points_2d[:,1] / 720\n",
    "\n",
    "        \n",
    "        rmse = torch.sqrt(mse_loss(points_2d, non_padded_input))\n",
    "        #loss_components['RMSE'].append(rmse.item())\n",
    "        loss += rmse\n",
    "   \n",
    "\n",
    "    return loss / batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785e189d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:35:37.674342Z",
     "iopub.status.busy": "2024-05-27T11:35:37.673978Z",
     "iopub.status.idle": "2024-05-27T11:35:37.687042Z",
     "shell.execute_reply": "2024-05-27T11:35:37.686175Z"
    },
    "papermill": {
     "duration": 1.308046,
     "end_time": "2024-05-27T11:35:37.689044",
     "exception": false,
     "start_time": "2024-05-27T11:35:36.380998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tune(num_epochs : int, lr : float, data_loader : DataLoader, use_court : bool, use_cam_params : bool, model):\n",
    "    \n",
    "    input_size = 100\n",
    "    output_size = 6\n",
    "    \n",
    "    if use_court:\n",
    "        input_size += 4*2\n",
    "    if use_cam_params:\n",
    "        input_size += 3*3 + 3 + 3*3 + 5\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "\n",
    "    #model = model_(input_size, output_size)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "\n",
    "\n",
    "        # Iterate over batches in the train_loader\n",
    "        for batch in tqdm(data_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "\n",
    "            projection, mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, shot_id_ = batch\n",
    "            \n",
    "            \n",
    "            concatted_input = projection.view(-1, 2*50)\n",
    "            \n",
    "            if use_court:\n",
    "                concatted_court = court_corners.view(-1, 4*2)\n",
    "                concatted_input = torch.cat((concatted_court, concatted_input), dim=1)\n",
    "            if use_cam_params:\n",
    "                camera_matrix = mtx.view(-1, 3*3)\n",
    "                translation_matrix = tvecs.view(-1, 3)\n",
    "                rotation = rotation_matrix.view(-1, 3*3)\n",
    "                distortion = dist.view(-1, 5)\n",
    "                concatted_params = torch.cat((camera_matrix, translation_matrix, rotation, distortion), dim=1)\n",
    "                concatted_input = torch.cat((concatted_params, concatted_input), dim=1)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(concatted_input.float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = batch_loss( outputs, mtx, dist, rvecs, tvecs, rotation_matrix, projection)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        # Print average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    return model, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5edf1a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:35:40.334851Z",
     "iopub.status.busy": "2024-05-27T11:35:40.334426Z",
     "iopub.status.idle": "2024-05-27T11:35:40.339092Z",
     "shell.execute_reply": "2024-05-27T11:35:40.338205Z"
    },
    "papermill": {
     "duration": 1.297696,
     "end_time": "2024-05-27T11:35:40.340962",
     "exception": false,
     "start_time": "2024-05-27T11:35:39.043266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#num_epochs : int, lr : float, data_loader : DataLoader, use_court : bool, use_cam_params : bool, model\n",
    "\n",
    "\n",
    "#model, loss = fine_tune(num_epochs = 20,\n",
    "#                       lr = 0.01,\n",
    "#                       data_loader = real_train_loader_NN,\n",
    "#                       use_court = True,\n",
    "#                       use_cam_params = False,\n",
    "#                       model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e2823",
   "metadata": {
    "papermill": {
     "duration": 1.378209,
     "end_time": "2024-05-27T11:35:43.101087",
     "exception": false,
     "start_time": "2024-05-27T11:35:41.722878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb850db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:35:45.767618Z",
     "iopub.status.busy": "2024-05-27T11:35:45.767291Z",
     "iopub.status.idle": "2024-05-27T11:35:45.791147Z",
     "shell.execute_reply": "2024-05-27T11:35:45.790229Z"
    },
    "papermill": {
     "duration": 1.399967,
     "end_time": "2024-05-27T11:35:45.793165",
     "exception": false,
     "start_time": "2024-05-27T11:35:44.393198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_shots(starting_params : list(), true_trajectories : list(), shot_ids : list, cam_matrices : list(), \n",
    "               dist_coeffs : list(), tvecs : list(), rot_matrices : list(), df : pd.DataFrame(), nr_shots : int):\n",
    "    \n",
    "    court_length, court_width, half_court_length, half_court_width, net_height_middle, net_height_sides = get_court_dimension()\n",
    "    \n",
    "    if nr_shots > len(shot_ids):\n",
    "        print('number of shots is too high')\n",
    "        return None\n",
    "    \n",
    "    for j in range(nr_shots):\n",
    "        \n",
    "        shot_id = shot_ids[j]\n",
    "        game = df.iloc[shot_id][\"game\"]\n",
    "        clip = df.iloc[shot_id][\"clip\"]\n",
    "        #start = df.iloc[shot_id][\"start\"]\n",
    "        start = 0\n",
    "        formatted_number = \"{:04d}.jpg\".format(start)\n",
    "        \n",
    "        preds = starting_params[j]\n",
    "        labels = true_trajectories[j][0].cpu()\n",
    "        \n",
    "        N = len(labels)\n",
    "        \n",
    "        # Find relevant camera parameters\n",
    "        camera_matrix = cam_matrices[j]\n",
    "        dist = dist_coeffs[j]\n",
    "        translation_vec = tvecs[j]\n",
    "        rotation_matrix = rot_matrices[j]\n",
    "        \n",
    "        # Make 3d trajectory and project down again\n",
    "        new_traj = create_3d_trajectory(preds.view(1, 6), N)\n",
    "        projected_path = project_points_torch(new_traj, rotation_matrix, translation_vec, camera_matrix, dist).detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        image_path = f\"/kaggle/input/tracknet-tennis/Dataset/{game}/{clip}/{formatted_number}\"\n",
    "        \n",
    "        #print(\"avg distance\", mse(projected_path, labels))\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        traj_pred = new_traj.cpu().numpy()\n",
    "        \n",
    "        #print('New Trajectory : ', projected_path)\n",
    "        \n",
    "        print(f'SHOT ID : {shot_id}\\n GAME : {game}\\n CLIP : {clip}')\n",
    "        \n",
    "        axs[0].imshow(img)\n",
    "        for i in range(len(projected_path)):\n",
    "            s = (i + 1) * 2 \n",
    "#             print(s)\n",
    "            axs[0].scatter(projected_path[i, 0], projected_path[i, 1], s=s, color=\"green\", label=\"Reprojected Shot\") \n",
    "            axs[0].scatter(labels[i,0], labels[i, 1], s=s, color=\"red\", label=\"True Shot\")\n",
    "            axs[0].set_title(\"Reprojection on real shot\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Plot the trajectory\n",
    "        axs[1] = fig.add_subplot(122, projection='3d')\n",
    "        for i in range(len(traj_pred)):\n",
    "            s = (i + 1) * 2  # Increase size with each iteration\n",
    "            color = 'green' if traj_pred[i, 2] > 0 else 'black'\n",
    "            axs[1].scatter(traj_pred[i, 0], traj_pred[i, 1], traj_pred[i, 2], s=s, c=color)        \n",
    "        \n",
    "        axs[1].set_xlabel('X')\n",
    "        axs[1].set_ylabel('Y')\n",
    "        axs[1].set_zlabel('Z')\n",
    "        axs[1].set_title('Tennis Shot Trajectory with Scatter Points')\n",
    "        axs[1].legend()\n",
    "        plot_tennis_court(axs[1])\n",
    "        \n",
    "        court_length = 23.77  # meters\n",
    "        court_width = 10.97  # meters\n",
    "        \n",
    "        axs[1].set_xlim(-half_court_length - 2, half_court_length+2)  # Set x-axis limits\n",
    "        axs[1].set_ylim(-half_court_length - 2, half_court_length+2)  # Set y-axis limits\n",
    "        axs[1].set_zlim(-1, 4)  # Set z-axis limits\n",
    "        \n",
    "#         axs[1].view_init(elev=1, azim=1)  # Change the elevation (up-down) and azimuth (left-right) angles\n",
    "\n",
    "        \n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fig2, axs2 = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        \n",
    "        angles = [(0, 0), (90, -90)]\n",
    "        for k in range(2):\n",
    "#             print(k)\n",
    "            angle = angles[k]\n",
    "            \n",
    "            # Plot the trajectory\n",
    "            axs2[k] = fig2.add_subplot(1, 2, k+1, projection='3d')\n",
    "            for i in range(len(traj_pred)):\n",
    "#                 print(s)\n",
    "                s = (i + 1) * 2  # Increase size with each iteration\n",
    "                color = 'green' if traj_pred[i, 2] > 0 else 'black'\n",
    "                axs2[k].scatter(traj_pred[i, 0], traj_pred[i, 1], traj_pred[i, 2], s=s, c=color)        \n",
    "\n",
    "            axs2[k].set_xlabel('X')\n",
    "            axs2[k].set_ylabel('Y')\n",
    "            axs2[k].set_zlabel('Z')\n",
    "            axs2[k].set_title('Tennis Shot Trajectory with Scatter Points')\n",
    "            axs2[k].legend()\n",
    "            plot_tennis_court(axs2[k])\n",
    "\n",
    "\n",
    "            axs2[k].set_xlim(-half_court_length - 2, half_court_length+2)  # Set x-axis limits\n",
    "            axs2[k].set_ylim(-half_court_length - 2, half_court_length+2)  # Set y-axis limits\n",
    "            axs2[k].set_zlim(-1, 4)  # Set z-axis limits\n",
    "\n",
    "\n",
    "            axs2[k].view_init(elev=angle[0], azim=angle[1])  # Change the elevation (up-down) and azimuth (left-right) angles\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf689791",
   "metadata": {
    "papermill": {
     "duration": 1.369088,
     "end_time": "2024-05-27T11:35:48.444047",
     "exception": false,
     "start_time": "2024-05-27T11:35:47.074959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2d trajectory\n",
    "# corners\n",
    "# Camera Parameters\n",
    "----------------------------------\n",
    "# start position 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c057df",
   "metadata": {
    "papermill": {
     "duration": 1.412498,
     "end_time": "2024-05-27T11:35:51.171060",
     "exception": false,
     "start_time": "2024-05-27T11:35:49.758562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GRU MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e1cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:14:28.931223Z",
     "iopub.status.busy": "2024-05-19T10:14:28.930518Z",
     "iopub.status.idle": "2024-05-19T10:14:28.941150Z",
     "shell.execute_reply": "2024-05-19T10:14:28.940427Z",
     "shell.execute_reply.started": "2024-05-19T10:14:28.931191Z"
    },
    "papermill": {
     "duration": 1.35638,
     "end_time": "2024-05-27T11:35:53.866862",
     "exception": false,
     "start_time": "2024-05-27T11:35:52.510482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        #self.fc1 = nn.Linear(non_traj_len, 128)\n",
    "        #self.bn1 = nn.BatchNorm1d(128)\n",
    "        #self.fc2_input_size = hidden_size + 128  # Size of concatenated tensors\n",
    "        #self.fc2 = nn.Linear(self.fc2_input_size, 256)  # Define fc2 with appropriate input size\n",
    "        #self.bn2 = nn.BatchNorm1d(256)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "        #self.bn3 = nn.BatchNorm1d(128)\n",
    "        #self.fc4 = nn.Linear(128, output_size)\n",
    "        \n",
    "        #print(hidden_size)\n",
    "        \n",
    "        \n",
    "        #self.num_layers = num_layers\n",
    "        #self.non_traj_len = non_traj_len\n",
    "        \n",
    "        #nn.init.constant_(self.fc1.weight, 0.1)\n",
    "        #nn.init.constant_(self.fc1.bias, 0)       # Initialize biases of fc1 to zero\n",
    "        ##nn.init.xavier_uniform_(self.fc2.weight)  # Xavier initialization for the weights of fc2\n",
    "        #nn.init.constant_(self.fc2.weight, 0.1)\n",
    "        #nn.init.constant_(self.fc2.bias, 0)       # Initialize biases of fc2 to zero\n",
    "        #nn.init.xavier_uniform_(self.fc3.weight)  # Xavier initialization for the weights of fc3\n",
    "        #nn.init.constant_(self.fc3.weight, 0.1)\n",
    "        #nn.init.constant_(self.fc3.bias, 0)       # Initialize biases of fc3 to zero\n",
    "        #nn.init.xavier_uniform_(self.fc4.weight)  # Xavier initialization for the weights of fc4\n",
    "        #nn.init.constant_(self.fc4.weight, 0.1)\n",
    "        #nn.init.constant_(self.fc4.bias, 0)       # Initialize biases of fc4 to zero\n",
    "        \n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "                #nn.init.constant_(param.data, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0.0)\n",
    "        \n",
    "    def forward(self, traj, h=None):\n",
    "\n",
    "        # put camera parameters through linear layer with Relu function\n",
    "        #rest = torch.relu(self.bn1(self.fc1(non_traj)))\n",
    "        \n",
    "        # Forward pass through the GRU layer\n",
    "        out, h = self.gru(traj, h)\n",
    "        \n",
    "        # Take only the output at the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        #concatenated = torch.cat((out, rest), dim=2)\n",
    "        #concatenated = torch.cat((out, rest.unsqueeze(1).repeat(1, out.size(1), 1)), dim=2)\n",
    "        #concatenated = concatenated.mean(dim=1)\n",
    "        #concatenated = torch.relu(self.bn2(self.fc2(concatenated)))\n",
    "        #concatenated = torch.relu(self.bn3(self.fc3(concatenated)))\n",
    "        #concatenated = self.fc4(concatenated)\n",
    "    \n",
    "        \n",
    "        # \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed53378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:30:09.023024Z",
     "iopub.status.busy": "2024-05-19T10:30:09.022645Z",
     "iopub.status.idle": "2024-05-19T10:30:09.037673Z",
     "shell.execute_reply": "2024-05-19T10:30:09.036830Z",
     "shell.execute_reply.started": "2024-05-19T10:30:09.022993Z"
    },
    "papermill": {
     "duration": 1.350896,
     "end_time": "2024-05-27T11:35:56.501808",
     "exception": false,
     "start_time": "2024-05-27T11:35:55.150912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_syn_GRU(num_epochs : int, lr : float, hidden_size : int, num_layers : int, \n",
    "                  use_court : bool, use_cam_params : bool, use_positions : bool, model_):\n",
    "\n",
    "    input_size = 2\n",
    "    output_size = 6\n",
    "    #hidden_size = hidden_size\n",
    "    #num_layers = num\n",
    "    \n",
    "\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    \n",
    "    model = model_(input_size, hidden_size, output_size, num_layers=1)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over batches in the train_loader\n",
    "        for batch in tqdm(syn_loader_GRU, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "\n",
    "            starting_params, projection,  mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, start_pos, end_pos = batch\n",
    "            \n",
    "            \n",
    "            #concatted_input = projection.view(-1, 2*50)\n",
    "            \n",
    "            if use_court:\n",
    "                concatted_court = court_corners.view(-1, 4*2).to(starting_params.device)\n",
    "                concatted_input = torch.cat((concatted_court, concatted_input), dim=1)\n",
    "            if use_cam_params:\n",
    "                camera_matrix = mtx.view(-1, 3*3).to(starting_params.device)\n",
    "                translation_matrix = tvecs.view(-1, 3).to(starting_params.device)\n",
    "                rotation = rotation_matrix.view(-1, 3*3).to(starting_params.device)\n",
    "                distortion = dist.view(-1, 5).to(starting_params.device)\n",
    "                concatted_params = torch.cat((camera_matrix, translation_matrix, rotation, distortion), dim=1)\n",
    "                concatted_input = torch.cat((concatted_params, concatted_input), dim=1).to(starting_params.device)\n",
    "            if use_positions:\n",
    "                concatted_positions = torch.cat((start_pos, end_pos)).view(-1, 3*2)\n",
    "                concatted_input = torch.cat((concatted_positions, concatted_input), dim=1).to(starting_params.device)\n",
    "                \n",
    "                \n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(projection.float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, starting_params)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        # Print average loss for the epoch\n",
    "        epoch_loss = running_loss / len(syn_loader.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65f627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:30:09.383966Z",
     "iopub.status.busy": "2024-05-19T10:30:09.383322Z",
     "iopub.status.idle": "2024-05-19T10:32:36.739541Z",
     "shell.execute_reply": "2024-05-19T10:32:36.738446Z",
     "shell.execute_reply.started": "2024-05-19T10:30:09.383940Z"
    },
    "papermill": {
     "duration": 1.35485,
     "end_time": "2024-05-27T11:35:59.139952",
     "exception": false,
     "start_time": "2024-05-27T11:35:57.785102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "model, loss = train_syn_GRU(num_epochs = 20, \n",
    "                            lr = 0.01,\n",
    "                            hidden_size = 128,\n",
    "                            num_layers = 1,\n",
    "                            use_court = False, \n",
    "                            use_cam_params = False,\n",
    "                            use_positions = False,\n",
    "                            model_ = SimpleGRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df2612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T10:32:36.742363Z",
     "iopub.status.busy": "2024-05-19T10:32:36.741628Z",
     "iopub.status.idle": "2024-05-19T10:32:36.754896Z",
     "shell.execute_reply": "2024-05-19T10:32:36.754095Z",
     "shell.execute_reply.started": "2024-05-19T10:32:36.742326Z"
    },
    "papermill": {
     "duration": 1.309943,
     "end_time": "2024-05-27T11:36:01.794565",
     "exception": false,
     "start_time": "2024-05-27T11:36:00.484622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def predict_GRU(data_loader : DataLoader, model):\n",
    "    \n",
    "    outputs = []\n",
    "    predicted_squares = []\n",
    "    true_squares = []\n",
    "    rmse_scores = []\n",
    "    mse_scores = []\n",
    "    reprojection_scores = []\n",
    "    true_trajectories = []\n",
    "    shot_ids = []\n",
    "    # Camera Parameters\n",
    "    camera_matrices = []\n",
    "    distortion_coefficients = []\n",
    "    translation_vectors = []\n",
    "    rotational_matrices = []\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            projection, mtx, dist, rvecs, tvecs, rotation_matrix, court_corners, shot_id_, player1, player2 = batch\n",
    "\n",
    "                \n",
    "            #print(concatted_input.float().device)\n",
    "\n",
    "            preds = model(projection.float())\n",
    "            \n",
    "            for i in range(len(batch[0])):\n",
    "                \n",
    "                camera_matrices.append(mtx[i])\n",
    "                distortion_coefficients.append(dist[i])\n",
    "                translation_vectors.append(tvecs[i])\n",
    "                rotational_matrices.append(rotation_matrix[i])\n",
    "                \n",
    "                curr_input = projection[i]\n",
    "                \n",
    "                shot_id = shot_id_[i]\n",
    "                shot_ids.append(shot_id)\n",
    "                \n",
    "                \n",
    "                mask = curr_input != -1\n",
    "                non_padded_input = curr_input[mask].view(1,-1,2)\n",
    "                \n",
    "\n",
    "                N = non_padded_input.shape[1]\n",
    "                \n",
    "\n",
    "                labels = non_padded_input#.detach().cpu().numpy()\n",
    "                \n",
    "                true_trajectories.append(labels)\n",
    "                \n",
    "                labels[:,:, 0] *= 1280\n",
    "                labels[:,:,1] *= 720\n",
    "                \n",
    "                \n",
    "                prediction = upscale_starting_params(preds[i])\n",
    "                outputs.append(prediction)\n",
    "                \n",
    "                new_traj = create_3d_trajectory(prediction.view(1, 6), N)\n",
    "                projected_path = project_points_torch(new_traj, rotation_matrix[i], tvecs[i], mtx[i], dist[i])\n",
    "                \n",
    "                \n",
    "                mse = mse_loss(projected_path, labels[0])\n",
    "                mse_scores.append(mse)\n",
    "                rmse = torch.sqrt(mse)\n",
    "                rmse_scores.append(rmse)\n",
    "            \n",
    "            \n",
    "    return outputs, rmse_scores, mse_scores, true_trajectories, shot_ids, camera_matrices, distortion_coefficients, translation_vectors, rotational_matrices\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f94cc5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-19T10:32:44.968713Z",
     "iopub.status.busy": "2024-05-19T10:32:44.968330Z",
     "iopub.status.idle": "2024-05-19T10:32:47.794622Z",
     "shell.execute_reply": "2024-05-19T10:32:47.793706Z",
     "shell.execute_reply.started": "2024-05-19T10:32:44.968681Z"
    },
    "papermill": {
     "duration": 1.297749,
     "end_time": "2024-05-27T11:36:04.456289",
     "exception": false,
     "start_time": "2024-05-27T11:36:03.158540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "model.to(device)\n",
    "(outputs, rmse_scores, mse_scores, true_trajectories, \n",
    " shot_ids, camera_matrices, \n",
    " distortion_coefficients, translation_vectors, \n",
    " rotational_matrices) = predict_GRU(data_loader = real_test_loader_GRU, \n",
    "                                    model = model)\n",
    "\n",
    "print('MEAN RMSE : ', torch.tensor(rmse_scores).mean())\n",
    "print('MEDIAN RMSE : ', torch.tensor(rmse_scores).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72149cf1",
   "metadata": {
    "papermill": {
     "duration": 1.288483,
     "end_time": "2024-05-27T11:36:07.162649",
     "exception": false,
     "start_time": "2024-05-27T11:36:05.874166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0c89f",
   "metadata": {
    "papermill": {
     "duration": 1.292672,
     "end_time": "2024-05-27T11:36:09.825907",
     "exception": false,
     "start_time": "2024-05-27T11:36:08.533235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b37c5c",
   "metadata": {
    "papermill": {
     "duration": 1.291682,
     "end_time": "2024-05-27T11:36:12.499670",
     "exception": false,
     "start_time": "2024-05-27T11:36:11.207988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4431943,
     "sourceId": 7737070,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4464105,
     "isSourceIdPinned": true,
     "sourceId": 7887373,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4478558,
     "sourceId": 7895915,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4666633,
     "sourceId": 8204503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4667195,
     "sourceId": 8336926,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4950884,
     "sourceId": 8459976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6033.110762,
   "end_time": "2024-05-27T11:36:15.588923",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-27T09:55:42.478161",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
